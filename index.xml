<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Orange</title>
    <link>https://ZeroAda.github.io/</link>
    <description>Recent content on Orange</description>
    <image>
      <url>https://ZeroAda.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E</url>
      <link>https://ZeroAda.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E</link>
    </image>
    <generator>Hugo -- gohugo.io</generator>
    <lastBuildDate>Wed, 20 Jul 2022 00:00:00 +0000</lastBuildDate><atom:link href="https://ZeroAda.github.io/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Summer camp: R Day3</title>
      <link>https://ZeroAda.github.io/posts/summer_camp3/</link>
      <pubDate>Wed, 20 Jul 2022 00:00:00 +0000</pubDate>
      
      <guid>https://ZeroAda.github.io/posts/summer_camp3/</guid>
      <description>Data analysis Warm up! We can use &amp;ldquo;describe&amp;rdquo; in psych package to see the number of participant, mean, std of a variable.
library(psych) describe(penguins$body_mass_g) result
vars n mean sd median trimmed mad min max range X1 1 342 4201.75 801.95 4050 4154.01 889.56 2700 6300 3600 skew kurtosis se X1 0.47 -0.74 43.36 What is TIDY DATA Every column is a variable Every row is an observation Every cell has one value It will benefit a lot if we deal with tidy data, for example, easy for data sharing, reproducible, easy to automate&amp;hellip; Data cleaning Remove data hierachically!</description>
    </item>
    
    <item>
      <title>Eligibility Trace, Andy Barto</title>
      <link>https://ZeroAda.github.io/posts/eligibility_traces/</link>
      <pubDate>Thu, 14 Jul 2022 00:00:00 +0000</pubDate>
      
      <guid>https://ZeroAda.github.io/posts/eligibility_traces/</guid>
      <description>Harry Klopf&amp;rsquo;s Hedonistic Hypothesis Neurons will maximize the local analog of pleasure and minimize the local analog of pain, i.e. it is a RL agent.
specific hypothesis When a neuron fires an action potential, all the contributing synapses become eligible to undergo changes in their efficacies, or weights.
If the action potential is followed within an appropriate time period by an increase in reward, an efficacies of all eligible synapses increase (or decrease in the case of punishment).</description>
    </item>
    
    <item>
      <title>Experience Replay and Data Efficiency</title>
      <link>https://ZeroAda.github.io/posts/experience_replay/</link>
      <pubDate>Thu, 14 Jul 2022 00:00:00 +0000</pubDate>
      
      <guid>https://ZeroAda.github.io/posts/experience_replay/</guid>
      <description>To make better use of data, we can use experience replay to increase data efficiency.
Experience replay We can put ${s,a,s&amp;rsquo;,r}$ pairs in the buffer and update Q using mini batch methods. To decrease noise in the replay, we average over several samples. (That&amp;rsquo;s why minibatch)
Infer-Collect framework </description>
    </item>
    
    <item>
      <title>Meta Reinforcment Learning</title>
      <link>https://ZeroAda.github.io/posts/meta_rl/</link>
      <pubDate>Thu, 14 Jul 2022 00:00:00 +0000</pubDate>
      
      <guid>https://ZeroAda.github.io/posts/meta_rl/</guid>
      <description>Meta learning system This part is based on Lilian Weng&amp;rsquo;s post meta rl.
Meta RL aims to adopt fast when new situations come.
To make it fast for RL, we introduce inductive bias in the system.
In meta-RL, we impose certain types of inductive biases from the task distribution and store them in memory. Which inductive bias to adopt at test time depends on the algorithm.
In the training setting, there will be a distribution of environments.</description>
    </item>
    
    <item>
      <title>Summer camp: R Day2</title>
      <link>https://ZeroAda.github.io/posts/summer_camp2/</link>
      <pubDate>Wed, 13 Jul 2022 00:00:00 +0000</pubDate>
      
      <guid>https://ZeroAda.github.io/posts/summer_camp2/</guid>
      <description>Create dataframe Create variables ## i. name names &amp;lt;- c(&amp;#34;Ada&amp;#34;,&amp;#34;Robert&amp;#34;,&amp;#34;Mia&amp;#34;) ## ii. age ages &amp;lt;- c(20,21,22) ## iii. Factor, so that you can add levels that not exist in the data year &amp;lt;- c(&amp;#34;Freshman&amp;#34;,&amp;#34;Sophomore&amp;#34;,&amp;#34;Junior&amp;#34;) year &amp;lt;- factor(year, levels=c(&amp;#34;Freshman&amp;#34;,&amp;#34;Sophomore&amp;#34;, &amp;#34;Junior&amp;#34;,&amp;#34;Senior&amp;#34;)) Create dataframe students &amp;lt;- data.frame(names,ages,year) Query a dataframe students$names Set working dictory get working dicrectory
getwd() set working directory
setwd() or go to &amp;ldquo;session&amp;rdquo; and set working directory
or build R file in the directory you want to work in</description>
    </item>
    
    <item>
      <title>Policy Gradient</title>
      <link>https://ZeroAda.github.io/posts/policy_gradient/</link>
      <pubDate>Tue, 12 Jul 2022 00:00:00 +0000</pubDate>
      
      <guid>https://ZeroAda.github.io/posts/policy_gradient/</guid>
      <description>Now, we want to parameterize the policy $\pi(a|s,\Theta) = Pr(A_t=a|S_t=s,\Theta_t=\Theta$ as long as it is differentiable. To find the best policy, we want to optimize according to $J$ using gradient ascent.
$$ \Theta_{t+1} = \Theta + \alpha \nabla J(\Theta_t) $$
Discrete action: Soft-max policy We compute preference $h(s,a,\Theta) = \Theta^T x(s,a)$. One of the most common way to parameterize policy is soft-max:
$$ \pi (a|s,\Theta) = \frac{e^{h(s,a,\Theta)}}{\sum_b e^{h(s,b,\Theta)}} $$
One advantage of soft max parameterization is that it can make the optimal determinstic policy.</description>
    </item>
    
    <item>
      <title>RL method overview</title>
      <link>https://ZeroAda.github.io/posts/monte_carlo/</link>
      <pubDate>Mon, 11 Jul 2022 00:00:00 +0000</pubDate>
      
      <guid>https://ZeroAda.github.io/posts/monte_carlo/</guid>
      <description>There are three kinds of RL in general. The first two are model-free methods.
Value: Parameterize value function; Estimate value function $q$ to generate the best policy. Q learning, SARSA
Policy: Parameterize policy gradient; Objective is to maximize the average return so that find the best policy directly; always combine with value-based method. Actor Critic
Model: Parameterize the model; Improve model accuracy; always combine with value-based method. Dyna-Q &amp;amp; Dyna-Q+</description>
    </item>
    
    <item>
      <title>PhD Application Diary</title>
      <link>https://ZeroAda.github.io/posts/phd_application_diary/</link>
      <pubDate>Wed, 06 Jul 2022 00:00:00 +0000</pubDate>
      
      <guid>https://ZeroAda.github.io/posts/phd_application_diary/</guid>
      <description>7.6 æ–¹å‘ï¼šCognitive Science, Neuroscience
æˆ‘æƒ³æŠŠä¸€åˆ‡éƒ½è®°å½•ä¸‹æ¥ï¼Œæ— è®ºç»“æœæ˜¯å¥½æ˜¯åã€‚æˆ‘å¼€å§‹å†™PSã€‚æœ‹å‹æ”¶åˆ°äº†cuhk cseçš„é¢è¯•ï¼Œç„¦è™‘åœ¨å¿ƒé‡Œç”Ÿé•¿ã€‚è¿™å‡ å¹´çš„æ¢ç´¢è®©æˆ‘è¶Šå‘ç†è§£è‡ªå·±ï¼Œå¦‚æœä¸æ˜¯æƒ³åš(curious)çš„äº‹æƒ…å°±ä¸ä¼šå»åšçš„ï¼›è€Œç»´æŒæˆ‘å·¥ä½œçš„åŠ¨åŠ›ï¼Œå¾ˆå¤šæ—¶å€™æ¥æºäºåˆä½œè€…ï¼Œcommunityçš„ä¿¡ä»»å’Œæ”¯æŒï¼›è¿˜æœ‰ä¸€äº›æ¥è‡ªäºï¼Œå¯¹è‡ªå·±specialçš„è®¤å®šã€‚ä»€ä¹ˆè®©æˆ‘æ„Ÿåˆ°special and interesting å‘¢ï¼ŸIf not MIT, would I accept a potential offer?
é™¤äº†å¤–åœ¨çš„æ¡ä»¶ï¼Œæ€æ ·æ‰ç®—æ˜¯å‡†å¤‡å¥½äº†ç”³è¯·PhDå‘¢ï¼Ÿ
äº†è§£è‡ªå·±æƒ³è§£å†³çš„å…·ä½“é—®é¢˜ çŸ¥é“è¿™ä¸ªé—®é¢˜æœ‰å“ªäº›potential answer çŸ¥é“è°åœ¨å°è¯•è§£å†³è¿™ä¸ªé—®é¢˜ 7.8 æˆ‘è¿™å‘¨å†™äº†Personal Statementï¼Œæƒ³ä¿®æ”¹ä¸€ä¸‹äº¤ç»™Joshå’Œå­¦é•¿ä¿®æ”¹ä¸€ä¸‹ã€‚ æˆ‘æ„Ÿåˆ°æˆ‘ç”³è¯·ä»€ä¹ˆä¸œè¥¿éƒ½å¾ˆéš¾å¾—åˆ°approvalï¼Œæ¯”å¦‚CNeuroå’Œstochastic labã€‚æˆ‘æœ‰ç‚¹æ‹…å¿ƒè‡ªå·±ç”³ä¸åˆ°å­¦æ ¡ã€‚æ€ä¹ˆæ ·æ‰èƒ½åšå¥½å‡†å¤‡å‘¢ï¼Ÿ ä»Šå¤©çœ‹åˆ°80000hoursç½‘ä¸Šâ€œå¦‚ä½•å»ºç«‹ä½ çš„career capitalâ€ï¼Œæåˆ°ä¸€ä¸ªé€‰æ‹©careerçš„æ€è·¯ï¼Œæ˜¯æ‰¾personal fitå’Œæœ‰å½±å“åŠ›çš„é—®é¢˜ä¹‹é—´çš„å¹³è¡¡ç‚¹ï¼Œæˆ‘çœ‹åˆ°é‡Œé¢æœ‰å‡ ä¸ªæˆ‘æ„Ÿå…´è¶£çš„ï¼š
AI safety
AI sentient
whole brain emulation å…¨è„‘æ¨¡æ‹Ÿ
ä¸ç®¡æ€ä¹ˆæ ·ï¼Œå¯¹è‡ªå·±æœªæ¥çš„é“è·¯ä¿æŒå¼€æ”¾ã€‚
7.11 æ˜¨å¤©å’Œä¸­ä»‹èŠäº†èŠï¼Œå¾—åˆ°ä¸€äº›ä¿¡æ¯
æš‘ç ”è¦å¦‚ä½•è¡¨ç°å¾—å¥½æ˜¯éœ€è¦åˆ»æ„çš„ï¼Œæˆ‘è‡ªå·±æƒ³äº†æƒ³è§‰å¾—éœ€è¦ weekly reportï¼ŒåŠæ—¶åé¦ˆ å¤šé—®é—®é¢˜ å’Œå¦ˆå¦ˆèŠäº†èŠï¼Œéœ€è¦çŸ¥é“ä¸­ä»‹çš„ä¸€äº›ä¿¡æ¯æ˜¯
é€‰æ ¡é€‰é¡¹ç›®ï¼Œæ˜¯ä¼šæ ¹æ®æˆ‘çš„æƒ…å†µç»™å‡ºprofå’Œé¡¹ç›®listï¼Œä»ç„¶ç”±æˆ‘è‡ªå·±ä¸€ä¸ªä¸ªçœ‹å—ï¼Ÿ 7.12 ä»Šå¤©çœ‹äº†æ¯•ä¸šè®¾è®¡çš„listï¼Œæ„Ÿè§‰æœ‰ç‚¹éš¾è¿‡ï¼Œä¸€æ˜¯å¥³æ€§æ•™æˆå¾ˆå°‘ï¼Œåªæœ‰ä¸€ä¸ªï¼ŒäºŒæ˜¯ç¬¦åˆæˆ‘research interestçš„å¾ˆå°‘å¾ˆå°‘ã€‚æˆ‘ä¸çŸ¥é“åšè¿™ä¸ªé¡¹ç›®èƒ½å¦å¢åŠ æˆ‘çš„èƒ½åŠ›ã€‚
8.14 ä¸çŸ¥ä¸è§‰è¿‡äº†ä¸€ä¸ªæœˆï¼Œæˆ‘å¥½åƒä»€ä¹ˆä¹Ÿæ²¡æœ‰å¹²ï¼Œåªæ˜¯å†™äº†ç¬¬ä¸€ç¨¿çš„æ–‡ä¹¦ã€‚ç°åœ¨æˆ‘è¦æ¥æ¢³ç†ä¸€ä¸‹æˆ‘çš„æ€è·¯ æˆ‘ç°åœ¨å¯¹æœªæ¥è§„åˆ’çš„å›°æƒ‘å’Œæ€è€ƒæ˜¯ï¼š
å¦‚æœæˆ‘æƒ³å°è¯•åˆ›ä¸šï¼Œåº”è¯¥åœ¨ä»€ä¹ˆé˜¶æ®µè¿›è¡Œå°è¯•ï¼Ÿæˆ‘è§‰å¾—åˆ›ä¸šéœ€è¦å¾ˆå¤šæ¡ä»¶ï¼Œä½†é¦–å…ˆæ˜¯æœ‰ä¸€ä¸ªideaï¼Œä¹‹åæ˜¯æ•¢äºå»åšå‡ºæ¥ï¼›å¦‚æœèƒ½åœ¨å¤§å­¦æœ‰ç»å†è¿™ç§å¤šæ¬¡åˆ›æ–°çš„æœºä¼šå°±å¾ˆå¥½ã€‚å¦‚æœè¦åˆ›ä¸šï¼Œæˆ‘ä¼šæƒ³åœ¨ä»€ä¹ˆé—®é¢˜ä¸Šåˆ›ä¸šï¼Ÿè¿™äº›äº§ä¸šçš„åˆ›ä¸šéœ€è¦æˆ‘å…·å¤‡åšå£«çš„çŸ¥è¯†å—ï¼Ÿ æ€§æ•™è‚² å¿ƒç†å’¨è¯¢ï¼Œå¿ƒç†å¥åº· æ¸¸æˆï¼Œgameï¼Œç»“åˆè®¤çŸ¥ åˆ›ä¸šæ˜¯å¾ˆéš¾å¾ˆéš¾çš„ è®¡ç®—è®¤çŸ¥ç§‘å­¦ï¼Œè®¡ç®—ç¥ç»ç§‘å­¦ï¼Œå¦‚æœæˆ‘è¯»å‡ºæ¥phDï¼Œåœ¨å·¥ä¸šç•Œæœ‰ä»€ä¹ˆå·¥ä½œæœºä¼šå—ï¼Ÿ techå…¬å¸ï¼šNeuralink, Apple, IBM, Deepmind: è¿™æ—¶å€™ï¼Œåšå£«çš„æŠ€èƒ½ç‚¹æ˜¯å¯è½¬ç§»çš„æŠ€èƒ½ç‚¹ è¯»å‡ºPhDï¼Œåœ¨å­¦æœ¯ç•Œçš„å·¥ä½œç”Ÿæ´»ï¼ˆæ•™æˆç­‰ç­‰ï¼‰æ˜¯æˆ‘æƒ³è¦çš„å—ï¼Ÿ postdoc - ç”³è¯·æ•™èŒ - tenure track - åŠ©ç†æ•™æˆ - å‰¯æ•™æˆ - æ­£æ•™æˆï¼šæˆ‘ç›®å‰çš„åˆ¤æ–­ï¼ŒAP çš„å·¥ä½œåŒ…æ‹¬æ•™å­¦ã€ç®¡ç†ã€ç§‘ç ”ï¼Œåœ¨tenure trackä¸Šä¼šå¾ˆå¿™ï¼Œæˆ‘ä¸çŸ¥é“å½“æ—¶è¿™æ˜¯ä¸æ˜¯æˆ‘æƒ³èµ°çš„è·¯ã€‚è¿™æ—¶å€™ï¼Œåšå£«çš„æŠ€èƒ½ç‚¹æ˜¯å»¶ç»­ä¸‹æ¥çš„ postdoc / non - ç ”ç©¶æ‰€ï¼šçº¯ç²¹ç ”ç©¶ï¼Œå¸¦é¢†ä¸‹ä¸€ä»£ç ”ç©¶è€…ç ”ç©¶ åšå£«æœŸé—´çš„ä½“éªŒä¹Ÿå¯¹ä¹‹åçš„é€‰æ‹©å¾ˆé‡è¦ï¼Œæ¯”å¦‚åšå£«æœŸé—´å¯¼å¸ˆåˆä½œä½“éªŒä¸å¥½ï¼Œå°±å¾ˆéš¾</description>
    </item>
    
    <item>
      <title>Summer camp: R Day1</title>
      <link>https://ZeroAda.github.io/posts/summer_camp1/</link>
      <pubDate>Wed, 06 Jul 2022 00:00:00 +0000</pubDate>
      
      <guid>https://ZeroAda.github.io/posts/summer_camp1/</guid>
      <description>Data type in the datasheet words: &amp;ldquo;California&amp;rdquo; categorical data: a / b / c logical: TRUE, FALSE number: 10 missing data: NA
Hot key run one chunk in the scripts:
ctrl + Enter run all chunks in the scripts:
ctrl + Enter + Shift Data type in R vector
vector &amp;lt;- c(&amp;#34;Ada&amp;#34;,&amp;#34;Emily&amp;#34;,&amp;#34;Jack&amp;#34;) if you combine different data types into one vector, you will get vector consist of string
vector &amp;lt;- c(TRUE,&amp;#34;Ada&amp;#34;,10) factor</description>
    </item>
    
    <item>
      <title>How to make decisions in a bandit game?</title>
      <link>https://ZeroAda.github.io/posts/sequence_decision/</link>
      <pubDate>Tue, 28 Jun 2022 00:00:00 +0000</pubDate>
      
      <guid>https://ZeroAda.github.io/posts/sequence_decision/</guid>
      <description>short notes for sequencial decision making</description>
    </item>
    
    <item>
      <title>Dynamic Programming for MDP</title>
      <link>https://ZeroAda.github.io/posts/dp_for_mdp/</link>
      <pubDate>Sun, 26 Jun 2022 00:00:00 +0000</pubDate>
      
      <guid>https://ZeroAda.github.io/posts/dp_for_mdp/</guid>
      <description>Before we delve into solving MDP by dynamic programming, let&amp;rsquo;s review concepts in MDP!
Markov Decision Process We can use MDP to describe our problems. It includes Action, State, Reward.
Markov Property The next state could be fully derived by only the current state. $$P(s_t,r_t|s_{t-1}, a_{t-1}, \dots s_0) = P(s_t,r_t|s_{t-1},a_{t-1})$$
Reward Hypothesis What we mean by goals and purposes can be well thought of as maximization of the expected value of the cumulative sum of a received scalar signal (reward).</description>
    </item>
    
    <item>
      <title>Fourier Transform</title>
      <link>https://ZeroAda.github.io/posts/fourier_transform/</link>
      <pubDate>Sat, 14 Aug 2021 00:00:00 +0000</pubDate>
      
      <guid>https://ZeroAda.github.io/posts/fourier_transform/</guid>
      <description>Cute Fourier Transformation coming ! Fourier transformation $X(t)$ is the amplitude-time function, we want to transform this into frequency domain. Typically, we want to decompose the function into several sine and cosine function with different frequency, phase and amplitude. $F$ represents the frequency we focus on.
$$X(F) = \int_{-\infty}^{+\infty} X(t) e ^{-i2\pi Ft}dt$$
The dot product verifies the similarity of the analysis function and the amplitude-time function.
Discrete Fourier transformation When we can only sample data from the signals, we replace the X with the folowwing.</description>
    </item>
    
    <item>
      <title>Mandelbrot Set</title>
      <link>https://ZeroAda.github.io/posts/mandelbrot_set/</link>
      <pubDate>Tue, 15 Sep 2020 00:00:00 +0000</pubDate>
      
      <guid>https://ZeroAda.github.io/posts/mandelbrot_set/</guid>
      <description>Use Matlab to create Mandelbrot Set.</description>
    </item>
    
    <item>
      <title>About</title>
      <link>https://ZeroAda.github.io/about/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://ZeroAda.github.io/about/</guid>
      <description>Hi, Iâ€™m Chenyi Li ğŸŠ. My current area of interest are Computational Neuroscience &amp;amp; Cognitive Neuroscience, Creative Programming. Feel free to talk with me!
Education âœï¸ Jan. 2022 - Aug. 2022 Visiting Student, University of California, Berkeley Core Course: Computational Model of Cognition(A+), Algorithm(B+), Artificial Intelligence Sept. 2019 - Jun. 2023 Bachelor of Engineering in Computer Engineering, The Chinese University of Hong Kong, Shenzhen CGPA: 3.58/4.0, Ranking: 17/74(22%) Core Course: Graduate Course Advanced Machine Learning, Computer Vision, Neural system(A-), Machine Learning(B+), Signal Processing(A-), Data Structure(B+), Linear Algebra(A), Operating System(A-), Optimization(B+) Dean&amp;rsquo;s List (2019-2020, 2020-2021, top 25%) Undergraduate Research Award (2020) Aug.</description>
    </item>
    
    
    <item>
      <title>Creative Programming</title>
      <link>https://ZeroAda.github.io/creative_programming/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://ZeroAda.github.io/creative_programming/</guid>
      <description>A list of all of creative programming piece at DES INV 23, UC Berkeley</description>
    </item>
    
    <item>
      <title>Music Garden</title>
      <link>https://ZeroAda.github.io/projects/music_garden/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://ZeroAda.github.io/projects/music_garden/</guid>
      <description>A garden of music</description>
    </item>
    
    
    <item>
      <title>What to do</title>
      <link>https://ZeroAda.github.io/projects/what_to_do/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      
      <guid>https://ZeroAda.github.io/projects/what_to_do/</guid>
      <description>ç”¨åæ€çš„å§¿æ€å¯¹æŠ—å†…å·</description>
    </item>
    
  </channel>
</rss>
