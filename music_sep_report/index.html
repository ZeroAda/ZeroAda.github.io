<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Music Source Separation | Orange</title><meta name=keywords content><meta name=description content="EIE3510 Signal Processing final project"><meta name=author content="Chenyi Li"><link rel=canonical href=https://ZeroAda.github.io/music_sep_report/><link crossorigin=anonymous href=/assets/css/stylesheet.min.c3e9e312489cf2251cb48d39a0c7b446b10f09c27a8f74cb80b221ed1fb07203.css integrity="sha256-w+njEkic8iUctI05oMe0RrEPCcJ6j3TLgLIh7R+wcgM=" rel="preload stylesheet" as=style><script defer crossorigin=anonymous src=/assets/js/highlight.min.b95bacdc39e37a332a9f883b1e78be4abc1fdca2bc1f2641f55e3cd3dabd4d61.js integrity="sha256-uVus3DnjejMqn4g7Hni+Srwf3KK8HyZB9V4809q9TWE=" onload=hljs.initHighlightingOnLoad()></script>
<link rel=icon href=https://ZeroAda.github.io/apple-touch-icon.png><link rel=icon type=image/png sizes=16x16 href=https://ZeroAda.github.io/apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=https://ZeroAda.github.io/apple-touch-icon.png><link rel=apple-touch-icon href=https://ZeroAda.github.io/apple-touch-icon.png><link rel=mask-icon href=https://ZeroAda.github.io/apple-touch-icon.png><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--hljs-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css integrity=sha384-KiWOvVjnN8qwAZbuQyWDIbfCLFhLXNETzBQjA/92pIowpC0d2O3nppDGQVgwd2nB crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js integrity=sha384-0fdwu/T/EQMsQlrHCCHoH10pkPLlKA1jL5dFyUOvB3lfeT2540/2g6YgSi2BL14p crossorigin=anonymous></script>
<script defer src=https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/contrib/auto-render.min.js integrity=sha384-+XBljXPPiv+OzfbB3cVmLHf4hdUFHlWNZN5spNQ7rmHTXpd7WvJum6fIACpNNfIR crossorigin=anonymous></script>
<script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1},{left:"\\[",right:"\\]",display:!0}],throwOnError:!1})})</script><script type=application/javascript>var doNotTrack=!1;doNotTrack||(function(e,t,n,s,o,i,a){e.GoogleAnalyticsObject=o,e[o]=e[o]||function(){(e[o].q=e[o].q||[]).push(arguments)},e[o].l=1*new Date,i=t.createElement(n),a=t.getElementsByTagName(n)[0],i.async=1,i.src=s,a.parentNode.insertBefore(i,a)}(window,document,"script","https://www.google-analytics.com/analytics.js","ga"),ga("create","UA-123-45","auto"),ga("send","pageview"))</script><meta property="og:title" content="Music Source Separation"><meta property="og:description" content="EIE3510 Signal Processing final project"><meta property="og:type" content="article"><meta property="og:url" content="https://ZeroAda.github.io/music_sep_report/"><meta property="og:image" content="https://ZeroAda.github.io/img/music_sep/Slide1.PNG"><meta property="article:section" content="projects"><meta property="og:site_name" content="Orange"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://ZeroAda.github.io/img/music_sep/Slide1.PNG"><meta name=twitter:title content="Music Source Separation"><meta name=twitter:description content="EIE3510 Signal Processing final project"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Projects","item":"https://ZeroAda.github.io/projects/"},{"@type":"ListItem","position":2,"name":"Music Source Separation","item":"https://ZeroAda.github.io/music_sep_report/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Music Source Separation","name":"Music Source Separation","description":"EIE3510 Signal Processing final project","keywords":[],"articleBody":"This is my final project of the class EIE3510 Signal Processing. I compare two traditional method and one deep learning methods. It’s a great journey to work with music!\nCode: Music Source separation\nPresentation: Music Source separation\nReport\nIntroduction The project aims to deal with a classical signal processing problem – music source separation. Specifically, separate the lead and the accompaniment in a song. In the project, I use music dataset MUSDB18HD, which contains 150 songs with different styles. Traditional methods (e.g. Sinusoidal model, REPET algorithm) and deep learning based method (e.g. Mask Inference) are applied. To quantitively measure the performance, I use evaluation metrics in Blind Source Separation (BSS), including Scaled Invariant Signal to Distortion ratio (SI-SDR), Signal to Artifacts ratio (SI-SAR), Signal to Interference ratio (SI-SIR). The deep learning method achieves the highest objective score. However, the result produced by REPET algorithm has the best sound effect. I mainly refer to the book \\cite{opensourceseparation:book} to implement the algorithms.The demo and code will be available in the github repository.\nProblem Given a clean mixture signal, we expect the lead signal and accompaniment signals. We assumes a linear mixture of the lead and accompaniment model as follows: $$ z[n] = x[n] + \\sum_{i=0}^{N-1}y_i[n] $$ where $z[n]$ is the mixture signal; $x[n]$ denotes the lead; $y = \\sum_{i=0}^{N-1}y_i[n]$ represents the sum of multiple musical instrument tracks.\nApproach The frame of separating music sources could be depicted as Figure 1. Firstly, represent the signal in a form such that the sources are separable. The most common approach is the Time-Frequency representation, which could be obtained by Short Time Fourier Transform in \\ref{sec:STFT}. The second stage is to detecting the assumed model in the TF representation, e.g. repetition pattern. The detected model and residual will be extracted by masking or filtering. Finally, the output signals will be transformed from frequency domain to time domain.\nFigure 1\nShort Time Fourier Transform An STFT is computed by Discrete Fourier Transform (DFT) of the signal in a small window. We denotes $X[k,m]$ as DFT of the short part that starts at sample $m$, windowed by a window of length less than or equal to $N$ samples at frequency $w_k = \\frac{2\\pi k}{N}$ $$ X[k,m] = \\sum_n x[n]w[n-m]e^{-jw_k(n-m)}, w_k = \\frac{2\\pi k}{N} $$ The $X[k,m]$ is put in the Time-Frequency matrix x at ($m,k$). $X[k,m]$ is complex number, but we usually use the absolute value of the real part to construct the magnitude spectrogram.\nModel and Separation Method To separate the lead and the accompaniment, the traditional methods usually construct a model based on some assumptions of the signals. Then, a series of transformations and masking will be applied to isolate signal sources. The traditional methods are model-based. I choose sinusoidal model, repetition model and harmoic-percussive model to analyze the mixture signals. Another stream of source separation method is driven by data, including methods in machine learning and deep learning. In the project, I use the mask inference method to train a mask, which extracts the vocals in spectrogram.\nSinusoidal Model and Analysis-Synthesis Method The sinusoidal model assumes that the vocals are mostly harmonic, so they could be represented by a set of harmonics with fundamental frequency $\\frac{2\\pi k_0}{N}$.\n$$ x[n] = \\sum_{r=1}^{R} A_r cos(\\frac{2\\pi k_0 n}{N}) $$ $$ X_r[k] = A_r \\sum w[n] \\frac{1}{2}(e^{\\frac{j2\\pi r k_0 n}{N} + e^{-\\frac{j2\\pi r k_0 n}{N}}} e^{\\frac{j2\\pi r k n}{N}} $$ where x[n] denotes that the leads could be modeled as the sum of sinusoidal functions with different amplitude and related frequency; $X_r[k]$ is the DFT of each sinusoidal function.\nPitch Detection One of the most commonly used pitch detection method is Autocorrelation. After computing the autocorrelation of a signal, the inverse of the point index with maximum ACF are regarded as the fundamental frequency of the signal. In this project, I use the Yin Algorithm in sonic visualizer to find the fundamental frequency of music in windows.\nSeparation To separate the leads and the accompaniment, we need to identify the fundamental frequency in the song by pitch detection algorithms. Then we reconstruct the set of harmonics in spectral domains whose frequencies are integer multiplications of the fundamental frequency and whose amplitudes are the spectral envelope of the recording. The estimated vocal spectrogram is then converted to waveform.\nRepetition Model and REPET algorithm The accompaniment in the background are assumed to have repetitions, so that the structure could be extracted and exploited to separate the sources.\nREpeating Pattern Extraction Technique (REPET) Figure 2\nTransform mixture signal from waveform $x$ to spectrogram $V$ and beat spectrum $b$. Find the fundamental repetition period $p$ in beat spectrum Divide $V$ into frame according to period $p$; take the median of frame as Repeating Segment $S$ Compare each frame with $S$ and take the minimum value element by element; construct a Repeating Spectrogram $W$ Compute Soft Mask $M$ as the $M = \\frac{W}{V}$ Harmonic Percussive Source Separation algorithms Combining two assumptions above, Harmonic Percussive Source Separation algorithm could extract the harmonic components and repetition pattern from original mixture. Harmonic Percussive Source Separation (HPSS) is derived accordingly.\nFigure 3\nTransform input signal $x$ to power spectrogram $y$ Construct Filtered spectrogram by applying horizontal and vertical median filtering. Given a matrix $B \\in R^{M\\times K}$, the harmonic and vertical median filter are as follows: $$ medfilt_h(B)(m,k) = median(B(m-l_h,k),\\dots,B(m+l_h,k)) $$ $$ medfilt_v(B)(m,k) = median(B(m,k-l_v),\\dots,B(m,k+l_v)) $$ We get the enhanced spectrogram $y_h = medianfilt_h(y)$ and $y_p=medianfilt_v(y)$ respectively. Compute binary mask for harmonic and vertical components. Each point (m,k) in the mask are as follows: Mask the magnitude spectrogram $X$ by $X \\cdot M_h$ and $X \\cdot M_p$\nCompute harmonic component $x_h$ and percussive component $x_p$ by inverse STFT The harmonic and vertical median filter could enhance the horizontal and vertical features of the signals. The enhancement could extract the corresponding pattern out.\nDeep Mask Inference Suppose there exists an ideal mask which could extract the vocal components in the spectrogram as shown in the Figure 4. With help of deep learning, I train the model to learn such a mask.\nFigure 4\nFigure 5\nArchitecture The neural network is composed of a batch normalization, two layers of LSTM, and a linear embedding, as depicted in Figure 5. The input and output size are the same $1724 \\times 256$. The amplitude to DB layer will compute $10log_10(x)$ could improve the training via scaling. The LSTM layer with dropout rate 0.3 could do nonlinear transform on the input signal. The feature number in hidden layer of LSTM is 50, so the output size of LSTM is $1724 \\times 100$. The linear embedding layer could map 100 features to 256 features.\nLoss function Suppose given the target source $Y$, trained mask $\\hat{M}$ and the mixture spectrogram $X$, we estimate the vocals by $\\hat{Y}=M \\cdot X$. The L1 loss function is: $$ L=||Y-\\hat{Y}|| $$\nTraining and testing Training data: 100 Testing data: 500 item Optimizer: Adam item Learning rate: 1e-3 Dataset MUSDB18 is dataset of 150 full tracks. Each songs is consist of 4 source, vocals, drums, bass, and others. All signals are stereophonic and encoded at frequency 44.1kHz. For convenience, I use 30s of each track to process. In deep learning method, I use 100 data to train the model. In the testing stage, 50 pieces of music are used to evaluate the performance for all the algorithms.\nTools I use sonic visualizer to visualize the spectrogram and analysis the music structure. For separation algorithms, I program in Python and exploit a signal processing package nussl.\nEvaluation Metrics According to \\cite{roux2018sdr}, the SI-SDR, SI-SAR and SI-SIR are more robust metrics compared with SDR, SAR and SIR. Suppose a mixture $x = s+n \\in R^{L}$, we estimate target source $s$ by $\\hat{s}$. We can decompose the $\\hat{s}$ to $e_{target} + e_{residual}$, leading to SI-SDR:\n$$ SI-SDR = 10 log_{10}(\\frac{|e_{target}|^2}{|e_{residual}|^2}) = 10 log_{10}(\\frac{|\\frac{\\hat{s}^{T}s}{|s|^2}s|^2}{|\\frac{\\hat{s}^{T}s}{|s|^2}s-s|^2}) $$\nThe $e_{residual}$ could be further decomposed to $e_{interference}+e_{artifacts}$. The $e_{interference}$ is the projection of $e_{res}$ into subspace spanned both by s and n. The SI-SIR and SI-SAR are defined as follows:\n$$ SI-SIR = 10 log_{10}(\\frac{|e_{target}|^2}{|e_{interference}|^2}) $$ $$ SI-SAR = 10 log_{10}(\\frac{|e_{target}|^2}{|e_{artifact}|^2}) $$\nResult I use a song from The Beatles. The waveform and spectrograms are shown in Figure 6. In the testing, I use the first song in test set as an example. Figure 6\nMusic Structure Analysis I use window size 1024. In Figure 7, the pink line represents the pitch detected. Pitch varies from 98Hz to 300 Hz. In Figure 8, the orange line represents the detected beat. The fundamental beat period is 0.546s. Figure 7\nFigure 8\nSeparation The following figures are the spectrogram and waveforms of the estimated vocals as well as accompaniments for the Beatles’ song.\nFigure 9 Please kindly note that the color in HPSS result is reverse of others. Figure 10\nREPET The REPET algorithm have a relatively good performance. Figure 11\nHPSS Please kindly note that the color in HPSS result is reverse of others.\nFigure 12\nDeep Mask Inference The training loss is depicted as follows. Figure 13\nThe result of separation for the Beatles’ song in the training dataset. Figure 14\nTesting In the testing set, I compare the four methods via subjective and objective measure on 50 data. The results are median of the metrics. While REPET has the best sound effect, the deep learning method get highest scores in the objective measure. The gap may be due to the nature of music. We evaluate the music by hearing it. There are some critical features in music, which could make a big difference in sound effect via a tiny fluctuation in number. Therefore, the subjective and objective measure could not accord with each other.\nAnother problem of all methods is that the vocals are mixed with the guitar melody. Two traditional methods assumes the accompaniments have repetition patterns. The assumption work well for the drums, but they could not be generalized to the accompaniments with melodies. For deep learning method, it could extract the common features of all songs, repetition. However, the guitar accompaniment does not have a fixed pattern in spectrogram, since every song has different harmonics. To solve the problem, more complex model for accompaniment (e.g. low rank assumption) should be exploited.\nFigure 15\nConclusion In the project, I use sinusoidal model, repetition model, harmoic-percussive model and Deep Mask Inference to solve the lead-accompaniment problem. The analysis and separation methods have sound performance in separating drums and vocals, but the music instrument with melodies could not be extracted. Although deep learning method have highest objective scores, it suffers from the variation of music so the sound effect is not up to expectation. Through this project, I have a basic understanding of the music source separation problem. I also learn different models regarding the leads and accompaniments. The programming implementation deepens my understanding various solutions to the problem.\nReference ","wordCount":"1803","inLanguage":"en","image":"https://ZeroAda.github.io/img/music_sep/Slide1.PNG","datePublished":"0001-01-01T00:00:00Z","dateModified":"0001-01-01T00:00:00Z","author":{"@type":"Person","name":"Chenyi Li"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://ZeroAda.github.io/music_sep_report/"},"publisher":{"@type":"Organization","name":"Orange","logo":{"@type":"ImageObject","url":"https://ZeroAda.github.io/apple-touch-icon.png"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://ZeroAda.github.io/ accesskey=h title="Chenyi Li (Alt + H)"><img src=https://ZeroAda.github.io/apple-touch-icon.png alt=logo aria-label=logo height=35>Chenyi Li</a>
<span class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></span></div><ul id=menu><li><a href=https://ZeroAda.github.io/about/ title=About><span>About</span></a></li><li><a href=https://ZeroAda.github.io/research/ title=Research><span>Research</span></a></li><li><a href=https://ZeroAda.github.io/presentation/ title=Presentation><span>Presentation</span></a></li><li><a href=https://ZeroAda.github.io/projects/ title=Projects><span>Projects</span></a></li><li><a href=https://ZeroAda.github.io/posts/ title=Posts><span>Posts</span></a></li><li><a href=https://ZeroAda.github.io/tags/ title=Tags><span>Tags</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://ZeroAda.github.io/>Home</a>&nbsp;»&nbsp;<a href=https://ZeroAda.github.io/projects/>Projects</a></div><h1 class=post-title>Music Source Separation</h1><div class=post-description>EIE3510 Signal Processing final project</div><div class=post-meta>Chenyi Li</div></header><div class=post-content><p>This is my final project of the class EIE3510 Signal Processing. I compare two traditional method and one deep learning methods. It&rsquo;s a great journey to work with music!</p><p>Code: <a href=https://github.com/ZeroAda/EIE3510-Music-Separation>Music Source separation</a></p><p>Presentation: <a href=https://ZeroAda.github.io/music_sep_pre>Music Source separation</a></p><p>Report</p><h2 id=introduction>Introduction<a hidden class=anchor aria-hidden=true href=#introduction>#</a></h2><p>The project aims to deal with a classical signal processing problem &ndash; music source separation. Specifically, separate the lead and the accompaniment in a song.
In the project, I use music dataset MUSDB18HD, which contains 150 songs with different styles. Traditional methods (e.g. Sinusoidal model, REPET algorithm) and deep learning based method (e.g. Mask Inference) are applied. To quantitively measure the performance, I use evaluation metrics in Blind Source Separation (BSS), including Scaled Invariant Signal to Distortion ratio (SI-SDR), Signal to Artifacts ratio (SI-SAR), Signal to Interference ratio (SI-SIR). The deep learning method achieves the highest objective score. However, the result produced by REPET algorithm has the best sound effect. I mainly refer to the book \cite{opensourceseparation:book} to implement the algorithms.The demo and code will be available in the github repository.</p><h2 id=problem>Problem<a hidden class=anchor aria-hidden=true href=#problem>#</a></h2><p>Given a clean mixture signal, we expect the lead signal and accompaniment signals. We assumes a linear mixture of the lead and accompaniment model as follows:
$$
z[n] = x[n] + \sum_{i=0}^{N-1}y_i[n]
$$
where $z[n]$ is the mixture signal; $x[n]$ denotes the lead; $y = \sum_{i=0}^{N-1}y_i[n]$ represents the sum of multiple musical instrument tracks.</p><h2 id=approach>Approach<a hidden class=anchor aria-hidden=true href=#approach>#</a></h2><p>The frame of separating music sources could be depicted as Figure 1. Firstly, represent the signal in a form such that the sources are separable. The most common approach is the Time-Frequency representation, which could be obtained by Short Time Fourier Transform in \ref{sec:STFT}. The second stage is to detecting the assumed model in the TF representation, e.g. repetition pattern. The detected model and residual will be extracted by masking or filtering. Finally, the output signals will be transformed from frequency domain to time domain.</p><p><img loading=lazy src=/img/music_sep/overal.png alt>
Figure 1</p><h3 id=short-time-fourier-transform>Short Time Fourier Transform<a hidden class=anchor aria-hidden=true href=#short-time-fourier-transform>#</a></h3><p>An STFT is computed by Discrete Fourier Transform (DFT) of the signal in a small window. We denotes $X[k,m]$ as DFT of the short part that starts at sample $m$, windowed by a window of length less than or equal to $N$ samples at frequency $w_k = \frac{2\pi k}{N}$
$$
X[k,m] = \sum_n x[n]w[n-m]e^{-jw_k(n-m)}, w_k = \frac{2\pi k}{N}
$$
The $X[k,m]$ is put in the Time-Frequency matrix <strong>x</strong> at ($m,k$). $X[k,m]$ is complex number, but we usually use the absolute value of the real part to construct the magnitude spectrogram.</p><h3 id=model-and-separation-method>Model and Separation Method<a hidden class=anchor aria-hidden=true href=#model-and-separation-method>#</a></h3><p>To separate the lead and the accompaniment, the traditional methods usually construct a model based on some assumptions of the signals. Then, a series of transformations and masking will be applied to isolate signal sources. The traditional methods are model-based. I choose <strong>sinusoidal model</strong>, <strong>repetition model</strong> and <strong>harmoic-percussive model</strong> to analyze the mixture signals. Another stream of source separation method is driven by data, including methods in machine learning and deep learning. In the project, I use the <strong>mask inference</strong> method to train a mask, which extracts the vocals in spectrogram.</p><h4 id=sinusoidal-model-and-analysis-synthesis-method>Sinusoidal Model and Analysis-Synthesis Method<a hidden class=anchor aria-hidden=true href=#sinusoidal-model-and-analysis-synthesis-method>#</a></h4><p>The sinusoidal model assumes that the vocals are mostly harmonic, so they could be represented by a set of harmonics with fundamental frequency $\frac{2\pi k_0}{N}$.</p><p>$$
x[n] = \sum_{r=1}^{R} A_r cos(\frac{2\pi k_0 n}{N})
$$
$$
X_r[k] = A_r \sum w[n] \frac{1}{2}(e^{\frac{j2\pi r k_0 n}{N} + e^{-\frac{j2\pi r k_0 n}{N}}} e^{\frac{j2\pi r k n}{N}}
$$
where x[n] denotes that the leads could be modeled as the sum of sinusoidal functions with different amplitude and related frequency; $X_r[k]$ is the DFT of each sinusoidal function.</p><h4 id=pitch-detection>Pitch Detection<a hidden class=anchor aria-hidden=true href=#pitch-detection>#</a></h4><p>One of the most commonly used pitch detection method is Autocorrelation. After computing the autocorrelation of a signal, the inverse of the point index with maximum ACF are regarded as the fundamental frequency of the signal. In this project, I use the Yin Algorithm in sonic visualizer to find the fundamental frequency of music in windows.</p><h4 id=separation>Separation<a hidden class=anchor aria-hidden=true href=#separation>#</a></h4><p>To separate the leads and the accompaniment, we need to identify the fundamental frequency in the song by pitch detection algorithms. Then we reconstruct the set of harmonics in spectral domains whose frequencies are integer multiplications of the fundamental frequency and whose amplitudes are the spectral envelope of the recording. The estimated vocal spectrogram is then converted to waveform.</p><h4 id=repetition-model-and-repet-algorithm>Repetition Model and REPET algorithm<a hidden class=anchor aria-hidden=true href=#repetition-model-and-repet-algorithm>#</a></h4><p>The accompaniment in the background are assumed to have <em>repetitions</em>, so that the structure could be extracted and exploited to separate the sources.</p><h4 id=repeating-pattern-extraction-technique-repet>REpeating Pattern Extraction Technique (REPET)<a hidden class=anchor aria-hidden=true href=#repeating-pattern-extraction-technique-repet>#</a></h4><p><img loading=lazy src=/img/music_sep/repet.JPG alt>
Figure 2</p><ol><li>Transform mixture signal from waveform $x$ to spectrogram $V$ and beat spectrum $b$. Find the fundamental repetition period $p$ in beat spectrum</li><li>Divide $V$ into frame according to period $p$; take the median of frame as Repeating Segment $S$</li><li>Compare each frame with $S$ and take the minimum value element by element; construct a Repeating Spectrogram $W$</li><li>Compute Soft Mask $M$ as the $M = \frac{W}{V}$</li></ol><h4 id=harmonic-percussive-source-separation-algorithms>Harmonic Percussive Source Separation algorithms<a hidden class=anchor aria-hidden=true href=#harmonic-percussive-source-separation-algorithms>#</a></h4><p>Combining two assumptions above, Harmonic Percussive Source Separation algorithm could extract the harmonic components and repetition pattern from original mixture. Harmonic Percussive Source Separation (HPSS) is derived accordingly.</p><p><img loading=lazy src=/img/music_sep/hpss.JPG alt>
Figure 3</p><ol><li>Transform input signal $x$ to power spectrogram $y$</li><li>Construct Filtered spectrogram by applying horizontal and vertical median filtering. Given a matrix $B \in R^{M\times K}$, the harmonic and vertical median filter are as follows:
$$
medfilt_h(B)(m,k) = median(B(m-l_h,k),\dots,B(m+l_h,k))
$$
$$
medfilt_v(B)(m,k) = median(B(m,k-l_v),\dots,B(m,k+l_v))
$$
We get the enhanced spectrogram $y_h = medianfilt_h(y)$ and $y_p=medianfilt_v(y)$ respectively.</li><li>Compute binary mask for harmonic and vertical components. Each point (m,k) in the mask are as follows:
<img loading=lazy src=/img/music_sep/formula.png alt></li></ol><p>Mask the magnitude spectrogram $X$ by $X \cdot M_h$ and $X \cdot M_p$</p><ol start=4><li>Compute harmonic component $x_h$ and percussive component $x_p$ by inverse STFT</li></ol><p>The harmonic and vertical median filter could enhance the horizontal and vertical features of the signals. The enhancement could extract the corresponding pattern out.</p><h4 id=deep-mask-inference>Deep Mask Inference<a hidden class=anchor aria-hidden=true href=#deep-mask-inference>#</a></h4><p>Suppose there exists an ideal mask which could extract the vocal components in the spectrogram as shown in the Figure 4. With help of deep learning, I train the model to learn such a mask.</p><p><img loading=lazy src=/img/music_sep/outline.png alt="Figure 4">
Figure 4</p><p><img loading=lazy src=/img/music_sep/dep.JPG alt="Figure 5">
Figure 5</p><h4 id=architecture>Architecture<a hidden class=anchor aria-hidden=true href=#architecture>#</a></h4><p>The neural network is composed of a batch normalization, two layers of LSTM, and a linear embedding, as depicted in Figure 5. The input and output size are the same $1724 \times 256$. The amplitude to DB layer will compute $10log_10(x)$ could improve the training via scaling. The LSTM layer with dropout rate 0.3 could do nonlinear transform on the input signal. The feature number in hidden layer of LSTM is 50, so the output size of LSTM is $1724 \times 100$. The linear embedding layer could map 100 features to 256 features.</p><h4 id=loss-function>Loss function<a hidden class=anchor aria-hidden=true href=#loss-function>#</a></h4><p>Suppose given the target source $Y$, trained mask $\hat{M}$ and the mixture spectrogram $X$, we estimate the vocals by $\hat{Y}=M \cdot X$. The L1 loss function is:
$$
L=||Y-\hat{Y}||
$$</p><h4 id=training-and-testing>Training and testing<a hidden class=anchor aria-hidden=true href=#training-and-testing>#</a></h4><ul><li>Training data: 100</li><li>Testing data: 500</li><li>item Optimizer: Adam</li><li>item Learning rate: 1e-3</li></ul><h3 id=dataset>Dataset<a hidden class=anchor aria-hidden=true href=#dataset>#</a></h3><p>MUSDB18 is dataset of 150 full tracks. Each songs is consist of 4 source, vocals, drums, bass, and others. All signals are stereophonic and encoded at frequency 44.1kHz. For convenience, I use 30s of each track to process. In deep learning method, I use 100 data to train the model. In the testing stage, 50 pieces of music are used to evaluate the performance for all the algorithms.</p><h3 id=tools>Tools<a hidden class=anchor aria-hidden=true href=#tools>#</a></h3><p>I use <em>sonic visualizer</em> to visualize the spectrogram and analysis the music structure. For separation algorithms, I program in Python and exploit a signal processing package <em>nussl</em>.</p><h3 id=evaluation-metrics>Evaluation Metrics<a hidden class=anchor aria-hidden=true href=#evaluation-metrics>#</a></h3><p>According to \cite{roux2018sdr}, the SI-SDR, SI-SAR and SI-SIR are more robust metrics compared with SDR, SAR and SIR. Suppose a mixture $x = s+n \in R^{L}$, we estimate target source $s$ by $\hat{s}$. We can decompose the $\hat{s}$ to $e_{target} + e_{residual}$, leading to SI-SDR:</p><p>$$
SI-SDR = 10 log_{10}(\frac{|e_{target}|^2}{|e_{residual}|^2}) = 10 log_{10}(\frac{|\frac{\hat{s}^{T}s}{|s|^2}s|^2}{|\frac{\hat{s}^{T}s}{|s|^2}s-s|^2})
$$</p><p>The $e_{residual}$ could be further decomposed to $e_{interference}+e_{artifacts}$. The $e_{interference}$ is the projection of $e_{res}$ into subspace spanned both by s and n. The SI-SIR and SI-SAR are defined as follows:</p><p>$$
SI-SIR = 10 log_{10}(\frac{|e_{target}|^2}{|e_{interference}|^2})
$$
$$
SI-SAR = 10 log_{10}(\frac{|e_{target}|^2}{|e_{artifact}|^2})
$$</p><h2 id=result>Result<a hidden class=anchor aria-hidden=true href=#result>#</a></h2><p>I use a song from The Beatles. The waveform and spectrograms are shown in Figure 6. In the testing, I use the first song in test set as an example.
<img loading=lazy src=/img/music_sep/ground_truth.png alt>
Figure 6</p><h3 id=music-structure-analysis>Music Structure Analysis<a hidden class=anchor aria-hidden=true href=#music-structure-analysis>#</a></h3><p>I use window size 1024. In Figure 7, the pink line represents the pitch detected. Pitch varies from 98Hz to 300 Hz. In Figure 8, the orange line represents the detected beat. The fundamental beat period is 0.546s.
<img loading=lazy src=/img/music_sep/pitch.JPG alt>
Figure 7</p><p><img loading=lazy src=/img/music_sep/beat.JPG alt>
Figure 8</p><h3 id=separation-1>Separation<a hidden class=anchor aria-hidden=true href=#separation-1>#</a></h3><p>The following figures are the spectrogram and waveforms of the estimated vocals as well as accompaniments for the Beatles&rsquo; song.</p><p><img loading=lazy src=/img/music_sep/train/repet.png alt>
Figure 9
Please kindly note that the color in HPSS result is reverse of others.
<img loading=lazy src=/img/music_sep/train/hpss.png alt>
Figure 10</p><h4 id=repet>REPET<a hidden class=anchor aria-hidden=true href=#repet>#</a></h4><p>The REPET algorithm have a relatively good performance.
<img loading=lazy src=/img/music_sep/train/repet.png alt>
Figure 11</p><h4 id=hpss>HPSS<a hidden class=anchor aria-hidden=true href=#hpss>#</a></h4><p>Please kindly note that the color in HPSS result is reverse of others.</p><p><img loading=lazy src=/img/music_sep/train/hpss.png alt>
Figure 12</p><h4 id=deep-mask-inference-1>Deep Mask Inference<a hidden class=anchor aria-hidden=true href=#deep-mask-inference-1>#</a></h4><p>The training loss is depicted as follows.
<img loading=lazy src=/img/music_sep/train/loss.png alt>
Figure 13</p><p>The result of separation for the Beatles&rsquo; song in the training dataset.
<img loading=lazy src=/img/music_sep/train/deep.png alt>
Figure 14</p><h4 id=testing>Testing<a hidden class=anchor aria-hidden=true href=#testing>#</a></h4><p>In the testing set, I compare the four methods via subjective and objective measure on 50 data. The results are median of the metrics. While REPET has the best sound effect, the deep learning method get highest scores in the objective measure. The gap may be due to the nature of music. We evaluate the music by hearing it. There are some critical features in music, which could make a big difference in sound effect via a tiny fluctuation in number. Therefore, the subjective and objective measure could not accord with each other.</p><p>Another problem of all methods is that the vocals are mixed with the guitar melody. Two traditional methods assumes the accompaniments have repetition patterns. The assumption work well for the drums, but they could not be generalized to the accompaniments with melodies. For deep learning method, it could extract the common features of all songs, repetition. However, the guitar accompaniment does not have a fixed pattern in spectrogram, since every song has different harmonics. To solve the problem, more complex model for accompaniment (e.g. low rank assumption) should be exploited.</p><p><img loading=lazy src=/img/music_sep/result.png alt>
Figure 15</p><h2 id=conclusion>Conclusion<a hidden class=anchor aria-hidden=true href=#conclusion>#</a></h2><p>In the project, I use <strong>sinusoidal model</strong>, <strong>repetition model</strong>, <strong>harmoic-percussive model</strong> and <strong>Deep Mask Inference</strong> to solve the lead-accompaniment problem. The analysis and separation methods have sound performance in separating drums and vocals, but the music instrument with melodies could not be extracted. Although deep learning method have highest objective scores, it suffers from the variation of music so the sound effect is not up to expectation. Through this project, I have a basic understanding of the music source separation problem. I also learn different models regarding the leads and accompaniments. The programming implementation deepens my understanding various solutions to the problem.</p><h2 id=reference>Reference<a hidden class=anchor aria-hidden=true href=#reference>#</a></h2></div><footer class=post-footer><div class=share-buttons><a target=_blank rel="noopener noreferrer" aria-label="share Music Source Separation on twitter" href="https://twitter.com/intent/tweet/?text=Music%20Source%20Separation&amp;url=https%3a%2f%2fZeroAda.github.io%2fmusic_sep_report%2f&amp;hashtags="><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM195.519 424.544c135.939.0 210.268-112.643 210.268-210.268.0-3.218.0-6.437-.153-9.502 14.406-10.421 26.973-23.448 36.935-38.314-13.18 5.824-27.433 9.809-42.452 11.648 15.326-9.196 26.973-23.602 32.49-40.92-14.252 8.429-30.038 14.56-46.896 17.931-13.487-14.406-32.644-23.295-53.946-23.295-40.767.0-73.87 33.104-73.87 73.87.0 5.824.613 11.494 1.992 16.858-61.456-3.065-115.862-32.49-152.337-77.241-6.284 10.881-9.962 23.601-9.962 37.088.0 25.594 13.027 48.276 32.95 61.456-12.107-.307-23.448-3.678-33.41-9.196v.92c0 35.862 25.441 65.594 59.311 72.49-6.13 1.686-12.72 2.606-19.464 2.606-4.751.0-9.348-.46-13.946-1.38 9.349 29.426 36.628 50.728 68.965 51.341-25.287 19.771-57.164 31.571-91.8 31.571-5.977.0-11.801-.306-17.625-1.073 32.337 21.15 71.264 33.41 112.95 33.41z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Music Source Separation on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=https%3a%2f%2fZeroAda.github.io%2fmusic_sep_report%2f&amp;title=Music%20Source%20Separation&amp;summary=Music%20Source%20Separation&amp;source=https%3a%2f%2fZeroAda.github.io%2fmusic_sep_report%2f"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Music Source Separation on reddit" href="https://reddit.com/submit?url=https%3a%2f%2fZeroAda.github.io%2fmusic_sep_report%2f&title=Music%20Source%20Separation"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Music Source Separation on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fZeroAda.github.io%2fmusic_sep_report%2f"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Music Source Separation on whatsapp" href="https://api.whatsapp.com/send?text=Music%20Source%20Separation%20-%20https%3a%2f%2fZeroAda.github.io%2fmusic_sep_report%2f"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Music Source Separation on telegram" href="https://telegram.me/share/url?text=Music%20Source%20Separation&amp;url=https%3a%2f%2fZeroAda.github.io%2fmusic_sep_report%2f"><svg viewBox="2 2 28 28"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg></a></div></footer></article></main><footer class=footer><span>&copy; 2023 <a href=https://ZeroAda.github.io/>Orange</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://git.io/hugopapermod rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg></a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>