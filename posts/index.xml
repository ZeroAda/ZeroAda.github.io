<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Posts on Orange</title>
    <link>https://ZeroAda.github.io/posts/</link>
    <description>Recent content in Posts on Orange</description>
    <image>
      <url>https://ZeroAda.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E</url>
      <link>https://ZeroAda.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E</link>
    </image>
    <generator>Hugo -- gohugo.io</generator>
    <lastBuildDate>Fri, 24 Mar 2023 00:00:00 +0000</lastBuildDate><atom:link href="https://ZeroAda.github.io/posts/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Graph Convolution Neural Network</title>
      <link>https://ZeroAda.github.io/posts/gcn/</link>
      <pubDate>Fri, 24 Mar 2023 00:00:00 +0000</pubDate>
      
      <guid>https://ZeroAda.github.io/posts/gcn/</guid>
      <description>GCN GCN originates from the article SEMI-SUPERVISED CLASSIFICATION WITH GRAPH CONVOLUTIONAL NETWORKS. Suppose there is a undirected graph G(V,E) with vertices V and edges E. It has an adjacency matrix A(VxV). Each node has a feature and the feature matrix is X(VxN) We want to classify the nodes into M categories using neural network f(X,A). $$ f(X,A) = softmax(\hat{A} ReLU(\hat{A} XW(0))W(1)) $$ where $$ \hat{A} = D^{-\frac{1}{2}}AD^{-\frac{1}{2}} $$ It computes the symmetrically normalized adjacency matrix of G.</description>
    </item>
    
    <item>
      <title>2022年书影札记</title>
      <link>https://ZeroAda.github.io/posts/2022_year/</link>
      <pubDate>Thu, 29 Dec 2022 00:00:00 +0000</pubDate>
      
      <guid>https://ZeroAda.github.io/posts/2022_year/</guid>
      <description>在阅读完《悉达多》后，我躲在我的小小房子里，吃了一瓣又一瓣蒸橘子，品味我最喜欢的书中的一段话
我听便灵魂与肉体的安排，去经历罪孽，去追逐肉欲与财富，去贪慕虚荣，以陷入最羞耻的绝望，以学会放弃挣扎，学会热爱世界。我不再将这个世杰与我所期待的、塑造的圆满世界比照，而是接受这个世界，爱它，属于它。——哦，乔文达，这就是我的一点思考与感悟。
这段话可以让修习多年而与世俗无染的乔文达沉思良久。悉达多说的是人间没有真相，而一切已经在万物之中显现。所以作为人，去经历一个人在世间的一切。
书 被讨厌的勇气。一本奠定本年学习基调的书。意识到过去的不能定义现在的我，学习课题分离。会再继续读的书，时时刻刻提醒自己。
必须有人开始。即使别人不合作，那也与你无关。我的意见就是这样。应该由你开始，不用去考虑别人是否合作。
克拉拉与太阳。一本探讨“人工智能是否有可能具有人性，成为人类”的书，后记有一段话：
没有了自私那下坠的重力，一切崇高、向上的人性也就虚无缥缈得失去了分量。自私是人类沉重的负担，但也许在并不遥远的未来，也会是人之所以为人的一个最重要的锚点吧.
赡养人类 赡养上帝
从零开始的女性主义
厌女
始于极限 上野千鹤子和铃木凉美的书信，多次被上野老师“野”到。好多大胆直爽通透的理解和表达！
“女人不能在没有爱的状态下做爱”……一试才发现容易的很
如果孩子对父母的渴求是一道终极的二选一——“爱还是理解”。……这不是理解，而是相信。这种相信的基础是爱。这种耿直的爱正是父母能够给予孩子最大的礼物。
近代性观念（只针对女性）规定“性和爱必须保持一致“。现在回想起来，浪漫爱意识形态是一种相当了的的伎俩，硬是把两种本不可能一致的东西凑在了一起。
因为长久以来，“常识”普遍认为，根据性的双重标准，男人只会把女人分为两类：为繁殖服务的女人（妻子和母亲）或为愉悦服务的女人（娼妇和情妇）。原来还出现了第三种类型，即“同事（包括上司和下属）属性的女人”！……照理说你只有当性对象的价值，可你却越界成了跟我平起平坐的同事，所以我要制裁你——这就是职场性骚扰的形成机制。
所谓婚姻，就是将自己身体的性使用权交给特定且唯一的异性，为其终生专属的契约。
why we sleep。一本在Berkeley临走前的睡前读物，没想到作者是Berkeley的老师。好读，让我更加重视睡眠了。睡眠不足驾驶如同酒驾。
睡眠不足，脑袋坏掉。
非暴力沟通。一本因为发生了冲突而迫切想学习的书。有《被讨厌的勇气》课题分离的精神内涵，是具体行动的指南。
克苏鲁神话1 《印斯茅斯的阴霾》看的我半夜睡不着了。人对未知的恐惧，恐惧的形态不可被描述。
它本身即是不得阅读之物。——爱伦·坡
悉达多
电影 悲惨世界 默默流泪
Alexander Hamilton 金色的一生，最喜欢《Satisfied》
红辣椒 The night to be remembered.
攻壳机动队 最喜欢《傀儡谣》。特别喜欢这个世界里漠然疏离的感觉，足以有空间来讨论人性。
密阳。 缓慢地看了李沧东的《薄荷糖》《燃烧》《诗》《绿洲》，终于以他的《密阳》为巡礼终章结束。
香水。始终相信嗅觉是最会铭记的感官，因为它直接连接大脑皮层。
青春变形记 皮克斯作品。风评嘲讽亚洲母女关系被刻板印象了，我倒觉得that&amp;rsquo;s it!
偶然与想象 短篇小说般的电影。
无人知晓 令人震惊的是基于真实事件改编。
何以为家 最后男孩说父母的罪状是把他生下来，看得心痛，但又不真实，像是借男孩之口说导演心中的话。
杀死比尔 血浆爱好者狂喜
天气之子
亚当斯一家
亚当斯一家的价值观 黑色幽默，极致嘲讽。年度最佳喜剧。
末路狂花 衣品真的不错。勇敢而美丽的女性。
分手的决心
银翼杀手 老片回顾，同《攻壳机动队》
头号玩家 简称宅男白日梦。
奥兰多 像诗歌一样的镜头，迷人的古一法师，浪漫主义，前后性别转变形成对比，作为女性的TA坚强又美丽。
Light Out 关灯后 新冠让我对恐怖片免疫了。</description>
    </item>
    
    <item>
      <title>Common Measure in Face Verification</title>
      <link>https://ZeroAda.github.io/posts/face_verification_metric/</link>
      <pubDate>Thu, 03 Nov 2022 00:00:00 +0000</pubDate>
      
      <guid>https://ZeroAda.github.io/posts/face_verification_metric/</guid>
      <description>In face verification task, we have several metrics to measure model performances.
Verification Rate (FR) under False Acceptance Rate First, we need to understand the false acceptance rate. There is a table showing reality and prediction.
false acceptance rate is defined as &amp;ldquo;the percentage of identification instances, in which unauthorised cases are incorrectly accepted&amp;rdquo;.
$$ FAR = \frac{FA}{FT} = \frac{False_Negative}{False_Negative+True_Negative} $$
Verification rate is the accuracy of a verification model.</description>
    </item>
    
    <item>
      <title>Summer camp: R Day3</title>
      <link>https://ZeroAda.github.io/posts/summer_camp3/</link>
      <pubDate>Wed, 20 Jul 2022 00:00:00 +0000</pubDate>
      
      <guid>https://ZeroAda.github.io/posts/summer_camp3/</guid>
      <description>Data analysis Warm up! We can use &amp;ldquo;describe&amp;rdquo; in psych package to see the number of participant, mean, std of a variable.
library(psych) describe(penguins$body_mass_g) result
vars n mean sd median trimmed mad min max range X1 1 342 4201.75 801.95 4050 4154.01 889.56 2700 6300 3600 skew kurtosis se X1 0.47 -0.74 43.36 What is TIDY DATA Every column is a variable Every row is an observation Every cell has one value It will benefit a lot if we deal with tidy data, for example, easy for data sharing, reproducible, easy to automate&amp;hellip; Data cleaning Remove data hierachically!</description>
    </item>
    
    <item>
      <title>Eligibility Trace, Andy Barto</title>
      <link>https://ZeroAda.github.io/posts/eligibility_traces/</link>
      <pubDate>Thu, 14 Jul 2022 00:00:00 +0000</pubDate>
      
      <guid>https://ZeroAda.github.io/posts/eligibility_traces/</guid>
      <description>Harry Klopf&amp;rsquo;s Hedonistic Hypothesis Neurons will maximize the local analog of pleasure and minimize the local analog of pain, i.e. it is a RL agent.
specific hypothesis When a neuron fires an action potential, all the contributing synapses become eligible to undergo changes in their efficacies, or weights.
If the action potential is followed within an appropriate time period by an increase in reward, an efficacies of all eligible synapses increase (or decrease in the case of punishment).</description>
    </item>
    
    <item>
      <title>Experience Replay and Data Efficiency</title>
      <link>https://ZeroAda.github.io/posts/experience_replay/</link>
      <pubDate>Thu, 14 Jul 2022 00:00:00 +0000</pubDate>
      
      <guid>https://ZeroAda.github.io/posts/experience_replay/</guid>
      <description>To make better use of data, we can use experience replay to increase data efficiency.
Experience replay We can put ${s,a,s&amp;rsquo;,r}$ pairs in the buffer and update Q using mini batch methods. To decrease noise in the replay, we average over several samples. (That&amp;rsquo;s why minibatch)
Infer-Collect framework </description>
    </item>
    
    <item>
      <title>Meta Reinforcment Learning</title>
      <link>https://ZeroAda.github.io/posts/meta_rl/</link>
      <pubDate>Thu, 14 Jul 2022 00:00:00 +0000</pubDate>
      
      <guid>https://ZeroAda.github.io/posts/meta_rl/</guid>
      <description>Meta learning system This part is based on Lilian Weng&amp;rsquo;s post meta rl.
Meta RL aims to adopt fast when new situations come.
To make it fast for RL, we introduce inductive bias in the system.
In meta-RL, we impose certain types of inductive biases from the task distribution and store them in memory. Which inductive bias to adopt at test time depends on the algorithm.
In the training setting, there will be a distribution of environments.</description>
    </item>
    
    <item>
      <title>Summer camp: R Day2</title>
      <link>https://ZeroAda.github.io/posts/summer_camp2/</link>
      <pubDate>Wed, 13 Jul 2022 00:00:00 +0000</pubDate>
      
      <guid>https://ZeroAda.github.io/posts/summer_camp2/</guid>
      <description>Create dataframe Create variables ## i. name names &amp;lt;- c(&amp;#34;Ada&amp;#34;,&amp;#34;Robert&amp;#34;,&amp;#34;Mia&amp;#34;) ## ii. age ages &amp;lt;- c(20,21,22) ## iii. Factor, so that you can add levels that not exist in the data year &amp;lt;- c(&amp;#34;Freshman&amp;#34;,&amp;#34;Sophomore&amp;#34;,&amp;#34;Junior&amp;#34;) year &amp;lt;- factor(year, levels=c(&amp;#34;Freshman&amp;#34;,&amp;#34;Sophomore&amp;#34;, &amp;#34;Junior&amp;#34;,&amp;#34;Senior&amp;#34;)) Create dataframe students &amp;lt;- data.frame(names,ages,year) Query a dataframe students$names Set working dictory get working dicrectory
getwd() set working directory
setwd() or go to &amp;ldquo;session&amp;rdquo; and set working directory
or build R file in the directory you want to work in</description>
    </item>
    
    <item>
      <title>Policy Gradient</title>
      <link>https://ZeroAda.github.io/posts/policy_gradient/</link>
      <pubDate>Tue, 12 Jul 2022 00:00:00 +0000</pubDate>
      
      <guid>https://ZeroAda.github.io/posts/policy_gradient/</guid>
      <description>Now, we want to parameterize the policy $\pi(a|s,\Theta) = Pr(A_t=a|S_t=s,\Theta_t=\Theta$ as long as it is differentiable. To find the best policy, we want to optimize according to $J$ using gradient ascent.
$$ \Theta_{t+1} = \Theta + \alpha \nabla J(\Theta_t) $$
Discrete action: Soft-max policy We compute preference $h(s,a,\Theta) = \Theta^T x(s,a)$. One of the most common way to parameterize policy is soft-max:
$$ \pi (a|s,\Theta) = \frac{e^{h(s,a,\Theta)}}{\sum_b e^{h(s,b,\Theta)}} $$
One advantage of soft max parameterization is that it can make the optimal determinstic policy.</description>
    </item>
    
    <item>
      <title>RL method overview</title>
      <link>https://ZeroAda.github.io/posts/monte_carlo/</link>
      <pubDate>Mon, 11 Jul 2022 00:00:00 +0000</pubDate>
      
      <guid>https://ZeroAda.github.io/posts/monte_carlo/</guid>
      <description>There are three kinds of RL in general. The first two are model-free methods.
Value: Parameterize value function; Estimate value function $q$ to generate the best policy. Q learning, SARSA
Policy: Parameterize policy gradient; Objective is to maximize the average return so that find the best policy directly; always combine with value-based method. Actor Critic
Model: Parameterize the model; Improve model accuracy; always combine with value-based method. Dyna-Q &amp;amp; Dyna-Q+</description>
    </item>
    
    <item>
      <title>PhD Application Diary</title>
      <link>https://ZeroAda.github.io/posts/phd_application_diary/</link>
      <pubDate>Wed, 06 Jul 2022 00:00:00 +0000</pubDate>
      
      <guid>https://ZeroAda.github.io/posts/phd_application_diary/</guid>
      <description>7.6 方向：Cognitive Science, Neuroscience
我想把一切都记录下来，无论结果是好是坏。我开始写PS。朋友收到了cuhk cse的面试，焦虑在心里生长。这几年的探索让我越发理解自己，如果不是想做(curious)的事情就不会去做的；而维持我工作的动力，很多时候来源于合作者，community的信任和支持；还有一些来自于，对自己special的认定。什么让我感到special and interesting 呢？If not MIT, would I accept a potential offer?
除了外在的条件，怎样才算是准备好了申请PhD呢？
了解自己想解决的具体问题 知道这个问题有哪些potential answer 知道谁在尝试解决这个问题 7.8 我这周写了Personal Statement，想修改一下交给Josh和学长修改一下。 我感到我申请什么东西都很难得到approval，比如CNeuro和stochastic lab。我有点担心自己申不到学校。怎么样才能做好准备呢？ 今天看到80000hours网上“如何建立你的career capital”，提到一个选择career的思路，是找personal fit和有影响力的问题之间的平衡点，我看到里面有几个我感兴趣的：
AI safety
AI sentient
whole brain emulation 全脑模拟
不管怎么样，对自己未来的道路保持开放。
7.11 昨天和中介聊了聊，得到一些信息
暑研要如何表现得好是需要刻意的，我自己想了想觉得需要 weekly report，及时反馈 多问问题 和妈妈聊了聊，需要知道中介的一些信息是
选校选项目，是会根据我的情况给出prof和项目list，仍然由我自己一个个看吗？ 7.12 今天看了毕业设计的list，感觉有点难过，一是女性教授很少，只有一个，二是符合我research interest的很少很少。我不知道做这个项目能否增加我的能力。
8.14 不知不觉过了一个月，我好像什么也没有干，只是写了第一稿的文书。现在我要来梳理一下我的思路 我现在对未来规划的困惑和思考是：
如果我想尝试创业，应该在什么阶段进行尝试？我觉得创业需要很多条件，但首先是有一个idea，之后是敢于去做出来；如果能在大学有经历这种多次创新的机会就很好。如果要创业，我会想在什么问题上创业？这些产业的创业需要我具备博士的知识吗？ 性教育 心理咨询，心理健康 游戏，game，结合认知 创业是很难很难的 计算认知科学，计算神经科学，如果我读出来phD，在工业界有什么工作机会吗？ tech公司：Neuralink, Apple, IBM, Deepmind: 这时候，博士的技能点是可转移的技能点 读出PhD，在学术界的工作生活（教授等等）是我想要的吗？ postdoc - 申请教职 - tenure track - 助理教授 - 副教授 - 正教授：我目前的判断，AP 的工作包括教学、管理、科研，在tenure track上会很忙，我不知道当时这是不是我想走的路。这时候，博士的技能点是延续下来的 postdoc / non - 研究所：纯粹研究，带领下一代研究者研究 博士期间的体验也对之后的选择很重要，比如博士期间导师合作体验不好，就很难</description>
    </item>
    
    <item>
      <title>Summer camp: R Day1</title>
      <link>https://ZeroAda.github.io/posts/summer_camp1/</link>
      <pubDate>Wed, 06 Jul 2022 00:00:00 +0000</pubDate>
      
      <guid>https://ZeroAda.github.io/posts/summer_camp1/</guid>
      <description>Data type in the datasheet words: &amp;ldquo;California&amp;rdquo; categorical data: a / b / c logical: TRUE, FALSE number: 10 missing data: NA
Hot key run one chunk in the scripts:
ctrl + Enter run all chunks in the scripts:
ctrl + Enter + Shift Data type in R vector
vector &amp;lt;- c(&amp;#34;Ada&amp;#34;,&amp;#34;Emily&amp;#34;,&amp;#34;Jack&amp;#34;) if you combine different data types into one vector, you will get vector consist of string
vector &amp;lt;- c(TRUE,&amp;#34;Ada&amp;#34;,10) factor</description>
    </item>
    
    <item>
      <title>How to make decisions in a bandit game?</title>
      <link>https://ZeroAda.github.io/posts/sequence_decision/</link>
      <pubDate>Tue, 28 Jun 2022 00:00:00 +0000</pubDate>
      
      <guid>https://ZeroAda.github.io/posts/sequence_decision/</guid>
      <description>short notes for sequencial decision making</description>
    </item>
    
    <item>
      <title>Dynamic Programming for MDP</title>
      <link>https://ZeroAda.github.io/posts/dp_for_mdp/</link>
      <pubDate>Sun, 26 Jun 2022 00:00:00 +0000</pubDate>
      
      <guid>https://ZeroAda.github.io/posts/dp_for_mdp/</guid>
      <description>Before we delve into solving MDP by dynamic programming, let&amp;rsquo;s review concepts in MDP!
Markov Decision Process We can use MDP to describe our problems. It includes Action, State, Reward.
Markov Property The next state could be fully derived by only the current state. $$P(s_t,r_t|s_{t-1}, a_{t-1}, \dots s_0) = P(s_t,r_t|s_{t-1},a_{t-1})$$
Reward Hypothesis What we mean by goals and purposes can be well thought of as maximization of the expected value of the cumulative sum of a received scalar signal (reward).</description>
    </item>
    
    <item>
      <title>Fourier Transform</title>
      <link>https://ZeroAda.github.io/posts/fourier_transform/</link>
      <pubDate>Sat, 14 Aug 2021 00:00:00 +0000</pubDate>
      
      <guid>https://ZeroAda.github.io/posts/fourier_transform/</guid>
      <description>Cute Fourier Transformation coming ! Fourier transformation $X(t)$ is the amplitude-time function, we want to transform this into frequency domain. Typically, we want to decompose the function into several sine and cosine function with different frequency, phase and amplitude. $F$ represents the frequency we focus on.
$$X(F) = \int_{-\infty}^{+\infty} X(t) e ^{-i2\pi Ft}dt$$
The dot product verifies the similarity of the analysis function and the amplitude-time function.
Discrete Fourier transformation When we can only sample data from the signals, we replace the X with the folowwing.</description>
    </item>
    
    <item>
      <title>My 1st post</title>
      <link>https://ZeroAda.github.io/posts/example/</link>
      <pubDate>Tue, 15 Sep 2020 11:30:03 +0000</pubDate>
      
      <guid>https://ZeroAda.github.io/posts/example/</guid>
      <description>Desc Text.</description>
    </item>
    
    <item>
      <title>Mandelbrot Set</title>
      <link>https://ZeroAda.github.io/posts/mandelbrot_set/</link>
      <pubDate>Tue, 15 Sep 2020 00:00:00 +0000</pubDate>
      
      <guid>https://ZeroAda.github.io/posts/mandelbrot_set/</guid>
      <description>Use Matlab to create Mandelbrot Set.</description>
    </item>
    
  </channel>
</rss>
