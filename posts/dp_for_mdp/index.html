<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Dynamic Programming for MDP | Orange</title><meta name=keywords content="Reinforcement Learning: an Introduction"><meta name=description content="Before we delve into solving MDP by dynamic programming, let&rsquo;s review concepts in MDP!
Markov Decision Process We can use MDP to describe our problems. It includes Action, State, Reward.
Markov Property The next state could be fully derived by only the current state. $$P(s_t,r_t|s_{t-1}, a_{t-1}, \dots s_0) = P(s_t,r_t|s_{t-1},a_{t-1})$$
Reward Hypothesis What we mean by goals and purposes can be well thought of as maximization of the expected value of the cumulative sum of a received scalar signal (reward)."><meta name=author content="Orange"><link rel=canonical href=https://canonical.url/to/page><link crossorigin=anonymous href=/assets/css/stylesheet.min.c3e9e312489cf2251cb48d39a0c7b446b10f09c27a8f74cb80b221ed1fb07203.css integrity="sha256-w+njEkic8iUctI05oMe0RrEPCcJ6j3TLgLIh7R+wcgM=" rel="preload stylesheet" as=style><script defer crossorigin=anonymous src=/assets/js/highlight.min.b95bacdc39e37a332a9f883b1e78be4abc1fdca2bc1f2641f55e3cd3dabd4d61.js integrity="sha256-uVus3DnjejMqn4g7Hni+Srwf3KK8HyZB9V4809q9TWE=" onload=hljs.initHighlightingOnLoad()></script>
<link rel=icon href=https://ZeroAda.github.io/apple-touch-icon.png><link rel=icon type=image/png sizes=16x16 href=https://ZeroAda.github.io/apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=https://ZeroAda.github.io/apple-touch-icon.png><link rel=apple-touch-icon href=https://ZeroAda.github.io/apple-touch-icon.png><link rel=mask-icon href=https://ZeroAda.github.io/apple-touch-icon.png><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--hljs-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css integrity=sha384-KiWOvVjnN8qwAZbuQyWDIbfCLFhLXNETzBQjA/92pIowpC0d2O3nppDGQVgwd2nB crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js integrity=sha384-0fdwu/T/EQMsQlrHCCHoH10pkPLlKA1jL5dFyUOvB3lfeT2540/2g6YgSi2BL14p crossorigin=anonymous></script>
<script defer src=https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/contrib/auto-render.min.js integrity=sha384-+XBljXPPiv+OzfbB3cVmLHf4hdUFHlWNZN5spNQ7rmHTXpd7WvJum6fIACpNNfIR crossorigin=anonymous></script>
<script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1},{left:"\\[",right:"\\]",display:!0}],throwOnError:!1})})</script><script type=application/javascript>var doNotTrack=!1;doNotTrack||(function(e,t,n,s,o,i,a){e.GoogleAnalyticsObject=o,e[o]=e[o]||function(){(e[o].q=e[o].q||[]).push(arguments)},e[o].l=1*new Date,i=t.createElement(n),a=t.getElementsByTagName(n)[0],i.async=1,i.src=s,a.parentNode.insertBefore(i,a)}(window,document,"script","https://www.google-analytics.com/analytics.js","ga"),ga("create","UA-123-45","auto"),ga("send","pageview"))</script><meta property="og:title" content="Dynamic Programming for MDP"><meta property="og:description" content="Before we delve into solving MDP by dynamic programming, let&rsquo;s review concepts in MDP!
Markov Decision Process We can use MDP to describe our problems. It includes Action, State, Reward.
Markov Property The next state could be fully derived by only the current state. $$P(s_t,r_t|s_{t-1}, a_{t-1}, \dots s_0) = P(s_t,r_t|s_{t-1},a_{t-1})$$
Reward Hypothesis What we mean by goals and purposes can be well thought of as maximization of the expected value of the cumulative sum of a received scalar signal (reward)."><meta property="og:type" content="article"><meta property="og:url" content="https://ZeroAda.github.io/posts/dp_for_mdp/"><meta property="og:image" content="https://ZeroAda.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta property="article:section" content="posts"><meta property="article:published_time" content="2022-06-26T00:00:00+00:00"><meta property="article:modified_time" content="2022-06-26T00:00:00+00:00"><meta property="og:site_name" content="Orange"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://ZeroAda.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta name=twitter:title content="Dynamic Programming for MDP"><meta name=twitter:description content="Before we delve into solving MDP by dynamic programming, let&rsquo;s review concepts in MDP!
Markov Decision Process We can use MDP to describe our problems. It includes Action, State, Reward.
Markov Property The next state could be fully derived by only the current state. $$P(s_t,r_t|s_{t-1}, a_{t-1}, \dots s_0) = P(s_t,r_t|s_{t-1},a_{t-1})$$
Reward Hypothesis What we mean by goals and purposes can be well thought of as maximization of the expected value of the cumulative sum of a received scalar signal (reward)."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://ZeroAda.github.io/posts/"},{"@type":"ListItem","position":2,"name":"Dynamic Programming for MDP","item":"https://ZeroAda.github.io/posts/dp_for_mdp/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Dynamic Programming for MDP","name":"Dynamic Programming for MDP","description":"Before we delve into solving MDP by dynamic programming, let\u0026rsquo;s review concepts in MDP!\nMarkov Decision Process We can use MDP to describe our problems. It includes Action, State, Reward.\nMarkov Property The next state could be fully derived by only the current state. $$P(s_t,r_t|s_{t-1}, a_{t-1}, \\dots s_0) = P(s_t,r_t|s_{t-1},a_{t-1})$$\nReward Hypothesis What we mean by goals and purposes can be well thought of as maximization of the expected value of the cumulative sum of a received scalar signal (reward).","keywords":["Reinforcement Learning: an Introduction"],"articleBody":"Before we delve into solving MDP by dynamic programming, let’s review concepts in MDP!\nMarkov Decision Process We can use MDP to describe our problems. It includes Action, State, Reward.\nMarkov Property The next state could be fully derived by only the current state. $$P(s_t,r_t|s_{t-1}, a_{t-1}, \\dots s_0) = P(s_t,r_t|s_{t-1},a_{t-1})$$\nReward Hypothesis What we mean by goals and purposes can be well thought of as maximization of the expected value of the cumulative sum of a received scalar signal (reward). Sutton\nReward could come from the environment.\ngoal-reward representation: 1 for goal, 0 otherwise action-penalty representation: 1 for not goal, 0 once goal reached action-penalty representation could encourage early finding of goals.\nactive research questions: How represent risk-sensitive behavior? How capture diversity in behaviors? Good matching for high-level human behaviors? ( create reward functions for ourselves? ) Valid policy A valid policy is a conditional distribution of actions over states. That is, given the fixed state, you should act with probability. policy fliping action, like “L,R,L,R…” is not valid, cuz it depends on the last action. But if we include the last action to state, it is valid.\noptimal policy An optimal policy is defined as the policy with the highest value possible value function for all states. At least one optimal policy always exists, but there may be more than one.\nDynamic Programming Policy evaluation is to evaluate the value functions of a policy. Policy control is to improve the policy. Since we have dynamics of the model $P$, we can use Dynamic programming to find the true value function and do improvement.\nValue function and Bellman equation We have defined the value function for each states if we follow the policy $\\pi$ as the expected return: $$ v_{\\pi}(s) = E_{\\pi}[G_t|S_t = s] = E_{\\pi}[\\sum_{k=0}^{\\infty} \\sigma^k R_{t+k+1}|S_t = s]$$\nThe state-value function for policy $\\pi$ is: $$q_{\\pi}(s,a)=E_{\\pi}[G_t|S_t = s,A_t=a]=E_{\\pi}[\\sum_{k=0}^{\\infty} \\sigma^k R_{t+k+1}|S_t = s,A_t=a]$$\nThere are recursive properties for value function and state-action functions. We will encounter them many times afterwards. The Bellman equation for $v_{\\pi}$ is: $$ v_{\\pi}(s) = \\sum_a \\pi(a|s) \\sum_{r,s’} p(r,s’|s,a)(r+\\sigma v_{\\pi}(s’)) $$ for all s $\\in$ S\nThe Bellman equation for $q_{\\pi}$ is:\n$$ q_{\\pi}(s,a) = \\sum_{r,s’} p(r,s’|s,a)(r+\\sigma \\sum_{a’} \\pi(a’|s’) q_{\\pi}(s’,a’)) $$ for all s,a $\\in$ S,A\nTo get more reward as possible, we want to find the optimal policy $\\pi*$ (maybe many) and they share one optimal value function and state-action function. They also have recursive properties. They are called Bellman optimality equations. $$q*(s,a) = max_{\\pi}q_{\\pi}(s,a)=\\pi(a|s) \\sum_{r,s’} p(r,s’|s,a)(r+\\sigma max_{a’} q*(s’,a’))$$\n$$v*(s ) = max_{\\pi}v_{\\pi}(s)=max_{a} \\sum_{r,s’} p(r,s’|s,a)(r+\\sigma v*(s’))$$\nOur goal is to find solutions so we can use greedy action to behave the best!\nIt is a equation system with n state(unknowns) and n equations. So we can solve it by hand or using a solver. We can only do this under the condition that\ndynamics of the environment is known computational resources are sufficient to complete the computation (so not feasible for large models) the states have the Markov property Now we are ready to see what we can do with DP! In general, it’s a method to iteratively or recurisvely achieve the result by dividing and reusing the problem.\nPolicy evalutaion We define the value function at $k$ iteration as $v_k(s)$ for each state.\n$$ v_{\\pi,k+1}(s) = \\sum_a \\pi(a|s) \\sum_{r,s’} p(r,s’|s,a)(r+\\sigma v_{\\pi,k}(s’)) $$\nWhen $v_{k,\\pi}$ achieves the true value function, it will not change.\nAlgorithm Input $\\pi$\nV = 0 V^ = 0 while True: diff = 0 for s in S: V^(s) = \\sum_a \\pi(a|s) \\sum_{r,s'} p(r,s'|s,a)(r+\\sigma V(s')) diff = max(diff, |V^(s)-V(s)|) V = V^ until diff \u003c theta Policy improvement Policy improvement theorem: if $$ q_{\\pi}(s,\\pi’(s)) \\geq q_{\\pi}(s,\\pi(s)) $$ for all s $\\in$ S then $\\pi’ \\geq \\pi$\nif $$ q_{\\pi}(s,\\pi’(s)) \u003e q_{\\pi}(s,\\pi(s)) $$ for all s $\\in$ S then $\\pi’ \u003e \\pi$\nSo if we improve policy by: $$ \\pi’(s) = argmax_a \\sum_{r,s’} p(s’,r|s,a) (r+\\sigma v(s)) $$ we can find a strictly better policy $\\pi’$ than $\\pi$\nPolicy iteration We bounce back and forth between policy evaluation and improvement until the policy is stable.\nAlgorithm Generalized Policy iteration It’s a general idea of letting policy evaluation and improvement iteract, regardless of granularity and details.\nValue iteration We can understand this as updating to compute Bellman optimality equation. It combines policy evaluation and improvement.\n$$ v_{\\pi,k+1}(s) = max_a\\sum_{r,s’} p(r,s’|s,a)(r+\\sigma v_{\\pi,k}(s’)) $$\nAsynchronous DP It’s a DP that do not update systematically, but in any order. But it needs to update all states.\nAlternatives to DP Monte Carlo Method sample a lot of sequences, and compute the mean $$ v(s) = E(G|s) $$\nBrute Force Search policy space to find the best one. Very time and space cost: $|A|^s$ Dynamic Programming\nDP use Bootstrapping to improve efficiency. Bootstrapping use the other states we compute to compute the current state value function.\nBetter than Monte Carlo Brute Force, but also encounter problems with exponentially many states.\n","wordCount":"812","inLanguage":"en","datePublished":"2022-06-26T00:00:00Z","dateModified":"2022-06-26T00:00:00Z","author":{"@type":"Person","name":"Orange"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://ZeroAda.github.io/posts/dp_for_mdp/"},"publisher":{"@type":"Organization","name":"Orange","logo":{"@type":"ImageObject","url":"https://ZeroAda.github.io/apple-touch-icon.png"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://ZeroAda.github.io/ accesskey=h title="Chenyi Li (Alt + H)"><img src=https://ZeroAda.github.io/apple-touch-icon.png alt=logo aria-label=logo height=35>Chenyi Li</a>
<span class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></span></div><ul id=menu><li><a href=https://ZeroAda.github.io/about/ title=About><span>About</span></a></li><li><a href=https://ZeroAda.github.io/research/ title=Research><span>Research</span></a></li><li><a href=https://ZeroAda.github.io/presentation/ title=Presentation><span>Presentation</span></a></li><li><a href=https://ZeroAda.github.io/projects/ title=Projects><span>Projects</span></a></li><li><a href=https://ZeroAda.github.io/posts/ title=Posts><span>Posts</span></a></li><li><a href=https://ZeroAda.github.io/tags/ title=Tags><span>Tags</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://ZeroAda.github.io/>Home</a>&nbsp;»&nbsp;<a href=https://ZeroAda.github.io/posts/>Posts</a></div><h1 class=post-title>Dynamic Programming for MDP</h1><div class=post-meta><span title='2022-06-26 00:00:00 +0000 UTC'>June 26, 2022</span>&nbsp;·&nbsp;4 min&nbsp;·&nbsp;Orange&nbsp;|&nbsp;<a href=https://github.com/%3cpath_to_repo%3e/content/posts/dp_for_mdp.md rel="noopener noreferrer" target=_blank>Suggest Changes</a></div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><ul><li><a href=#markov-decision-process aria-label="Markov Decision Process">Markov Decision Process</a><ul><li><a href=#markov-property aria-label="Markov Property">Markov Property</a></li><li><a href=#reward-hypothesis aria-label="Reward Hypothesis">Reward Hypothesis</a></li><li><a href=#valid-policy aria-label="Valid policy">Valid policy</a></li><li><a href=#optimal-policy aria-label="optimal policy">optimal policy</a></li></ul></li><li><a href=#dynamic-programming aria-label="Dynamic Programming">Dynamic Programming</a><ul><li><a href=#value-function-and-bellman-equation aria-label="Value function and Bellman equation">Value function and Bellman equation</a></li><li><a href=#policy-evalutaion aria-label="Policy evalutaion">Policy evalutaion</a></li><li><a href=#policy-improvement aria-label="Policy improvement">Policy improvement</a></li><li><a href=#policy-iteration aria-label="Policy iteration">Policy iteration</a></li><li><a href=#generalized-policy-iteration aria-label="Generalized Policy iteration">Generalized Policy iteration</a><ul><li><a href=#value-iteration aria-label="Value iteration">Value iteration</a></li><li><a href=#asynchronous-dp aria-label="Asynchronous DP">Asynchronous DP</a></li></ul></li></ul></li><li><a href=#alternatives-to-dp aria-label="Alternatives to DP">Alternatives to DP</a></li></ul></div></details></div><div class=post-content><p>Before we delve into solving MDP by dynamic programming, let&rsquo;s review concepts in MDP!</p><h2 id=markov-decision-process>Markov Decision Process<a hidden class=anchor aria-hidden=true href=#markov-decision-process>#</a></h2><p>We can use MDP to describe our problems. It includes <em>Action, State, Reward</em>.</p><h3 id=markov-property>Markov Property<a hidden class=anchor aria-hidden=true href=#markov-property>#</a></h3><p>The next state could be fully derived by only the current state.
$$P(s_t,r_t|s_{t-1}, a_{t-1}, \dots s_0) = P(s_t,r_t|s_{t-1},a_{t-1})$$</p><h3 id=reward-hypothesis>Reward Hypothesis<a hidden class=anchor aria-hidden=true href=#reward-hypothesis>#</a></h3><blockquote><p>What we mean by goals and purposes can be well thought of as maximization of the expected value of the cumulative sum of a received scalar signal (reward).
Sutton</p></blockquote><p>Reward could come from the environment.</p><ul><li>goal-reward representation: 1 for goal, 0 otherwise</li><li>action-penalty representation: 1 for not goal, 0 once goal reached</li></ul><p><em>action-penalty representation</em> could encourage early finding of goals.</p><ul><li>active research questions:</li></ul><ol><li>How represent risk-sensitive behavior?</li><li>How capture diversity in behaviors?</li><li>Good matching for high-level human behaviors? ( create reward functions for ourselves? )</li></ol><h3 id=valid-policy>Valid policy<a hidden class=anchor aria-hidden=true href=#valid-policy>#</a></h3><p>A valid policy is a conditional distribution of actions over states. That is, given the fixed state, you should act with probability.
policy fliping action, like &ldquo;L,R,L,R&mldr;&rdquo; is not valid, cuz it depends on the last action. But if we include the last action to state, it is valid.</p><h3 id=optimal-policy>optimal policy<a hidden class=anchor aria-hidden=true href=#optimal-policy>#</a></h3><p>An optimal policy is defined as the policy with the highest value possible value function for all states. At least one optimal policy always exists, but there may be more than one.</p><h2 id=dynamic-programming>Dynamic Programming<a hidden class=anchor aria-hidden=true href=#dynamic-programming>#</a></h2><p>Policy evaluation is to evaluate the value functions of a policy. Policy control is to improve the policy. Since we have dynamics of the model $P$, we can use Dynamic programming to find the true value function and do improvement.</p><h3 id=value-function-and-bellman-equation>Value function and Bellman equation<a hidden class=anchor aria-hidden=true href=#value-function-and-bellman-equation>#</a></h3><p>We have defined the <strong>value function</strong> for each states if we follow the policy $\pi$ as the expected return:
$$ v_{\pi}(s) = E_{\pi}[G_t|S_t = s] = E_{\pi}[\sum_{k=0}^{\infty} \sigma^k R_{t+k+1}|S_t = s]$$</p><p>The <strong>state-value function</strong> for policy $\pi$ is:
$$q_{\pi}(s,a)=E_{\pi}[G_t|S_t = s,A_t=a]=E_{\pi}[\sum_{k=0}^{\infty} \sigma^k R_{t+k+1}|S_t = s,A_t=a]$$</p><p>There are recursive properties for value function and state-action functions. We will encounter them many times afterwards.
The Bellman equation for $v_{\pi}$ is:
$$
v_{\pi}(s) = \sum_a \pi(a|s) \sum_{r,s&rsquo;} p(r,s&rsquo;|s,a)(r+\sigma v_{\pi}(s&rsquo;))
$$
for all s $\in$ S</p><p>The Bellman equation for $q_{\pi}$ is:</p><p>$$
q_{\pi}(s,a) = \sum_{r,s&rsquo;} p(r,s&rsquo;|s,a)(r+\sigma \sum_{a&rsquo;} \pi(a&rsquo;|s&rsquo;) q_{\pi}(s&rsquo;,a&rsquo;))
$$
for all s,a $\in$ S,A</p><p>To get more reward as possible, we want to find the optimal policy $\pi*$ (maybe many) and they share one optimal value function and state-action function. They also have recursive properties. They are called Bellman optimality equations.
$$q*(s,a) = max_{\pi}q_{\pi}(s,a)=\pi(a|s) \sum_{r,s&rsquo;} p(r,s&rsquo;|s,a)(r+\sigma max_{a&rsquo;} q*(s&rsquo;,a&rsquo;))$$</p><p>$$v*(s ) = max_{\pi}v_{\pi}(s)=max_{a} \sum_{r,s&rsquo;} p(r,s&rsquo;|s,a)(r+\sigma v*(s&rsquo;))$$</p><p>Our goal is to find solutions so we can use greedy action to behave the best!</p><p>It is a equation system with n state(unknowns) and n equations. So we can solve it by hand or using a solver. We can only do this under the condition that</p><ol><li>dynamics of the environment is known</li><li>computational resources are sufficient to complete the computation (so not feasible for large models)</li><li>the states have the Markov property</li></ol><p>Now we are ready to see what we can do with DP! In general, it&rsquo;s a method to iteratively or recurisvely achieve the result by dividing and reusing the problem.</p><h3 id=policy-evalutaion>Policy evalutaion<a hidden class=anchor aria-hidden=true href=#policy-evalutaion>#</a></h3><p>We define the value function at $k$ iteration as $v_k(s)$ for each state.</p><p>$$
v_{\pi,k+1}(s) = \sum_a \pi(a|s) \sum_{r,s&rsquo;} p(r,s&rsquo;|s,a)(r+\sigma v_{\pi,k}(s&rsquo;))
$$</p><p>When $v_{k,\pi}$ achieves the true value function, it will not change.</p><p><strong>Algorithm</strong>
Input $\pi$</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>V <span style=color:#f92672>=</span> <span style=color:#ae81ff>0</span>
</span></span><span style=display:flex><span>V<span style=color:#f92672>^</span> <span style=color:#f92672>=</span> <span style=color:#ae81ff>0</span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>while</span> <span style=color:#66d9ef>True</span>:
</span></span><span style=display:flex><span>    diff <span style=color:#f92672>=</span> <span style=color:#ae81ff>0</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>for</span> s <span style=color:#f92672>in</span> S:
</span></span><span style=display:flex><span>        V<span style=color:#f92672>^</span>(s) <span style=color:#f92672>=</span>  \sum_a \pi(a<span style=color:#f92672>|</span>s) \sum_{r,s<span style=color:#e6db74>&#39;} p(r,s&#39;</span><span style=color:#f92672>|</span>s,a)(r<span style=color:#f92672>+</span>\sigma V(s<span style=color:#e6db74>&#39;))</span>
</span></span><span style=display:flex><span>        diff <span style=color:#f92672>=</span> max(diff, <span style=color:#f92672>|</span>V<span style=color:#f92672>^</span>(s)<span style=color:#f92672>-</span>V(s)<span style=color:#f92672>|</span>)
</span></span><span style=display:flex><span>    V <span style=color:#f92672>=</span> V<span style=color:#f92672>^</span>
</span></span><span style=display:flex><span>until diff <span style=color:#f92672>&lt;</span> theta
</span></span></code></pre></div><h3 id=policy-improvement>Policy improvement<a hidden class=anchor aria-hidden=true href=#policy-improvement>#</a></h3><p>Policy improvement theorem:
if
$$
q_{\pi}(s,\pi&rsquo;(s)) \geq q_{\pi}(s,\pi(s))
$$
for all s $\in$ S then $\pi&rsquo; \geq \pi$</p><p>if
$$
q_{\pi}(s,\pi&rsquo;(s)) > q_{\pi}(s,\pi(s))
$$
for all s $\in$ S then $\pi&rsquo; > \pi$</p><p>So if we improve policy by:
$$
\pi&rsquo;(s) = argmax_a \sum_{r,s&rsquo;} p(s&rsquo;,r|s,a) (r+\sigma v(s))
$$
we can find a strictly better policy $\pi&rsquo;$ than $\pi$</p><h3 id=policy-iteration>Policy iteration<a hidden class=anchor aria-hidden=true href=#policy-iteration>#</a></h3><p>We bounce back and forth between policy evaluation and improvement until the policy is stable.</p><p><img loading=lazy src=/img/rl/policy_iteration3.png alt="policy iteration"></p><p><strong>Algorithm</strong>
<img loading=lazy src=/img/rl/policy_iteration4.png alt></p><h3 id=generalized-policy-iteration>Generalized Policy iteration<a hidden class=anchor aria-hidden=true href=#generalized-policy-iteration>#</a></h3><p>It&rsquo;s a general idea of letting policy evaluation and improvement iteract, regardless of granularity and details.</p><h4 id=value-iteration>Value iteration<a hidden class=anchor aria-hidden=true href=#value-iteration>#</a></h4><p>We can understand this as updating to compute Bellman optimality equation. It combines policy evaluation and improvement.</p><p>$$
v_{\pi,k+1}(s) = max_a\sum_{r,s&rsquo;} p(r,s&rsquo;|s,a)(r+\sigma v_{\pi,k}(s&rsquo;))
$$</p><p><img loading=lazy src=/img/rl/value_iteration.png alt></p><h4 id=asynchronous-dp>Asynchronous DP<a hidden class=anchor aria-hidden=true href=#asynchronous-dp>#</a></h4><p>It&rsquo;s a DP that do not update systematically, but in any order. But it needs to update <em>all</em> states.</p><h2 id=alternatives-to-dp>Alternatives to DP<a hidden class=anchor aria-hidden=true href=#alternatives-to-dp>#</a></h2><p><strong>Monte Carlo Method</strong>
sample a lot of sequences, and compute the mean
$$
v(s) = E(G|s)
$$</p><p><strong>Brute Force</strong>
Search policy space to find the best one. Very time and space cost: $|A|^s$
<strong>Dynamic Programming</strong></p><p>DP use <strong>Bootstrapping</strong> to improve efficiency. <strong>Bootstrapping</strong> use the other states we compute to compute the current state value function.</p><p>Better than <strong>Monte Carlo</strong> <strong>Brute Force</strong>, but also encounter problems with exponentially many states.</p></div><footer class=post-footer><ul class=post-tags><li><a href=https://ZeroAda.github.io/tags/reinforcement-learning-an-introduction/>Reinforcement Learning: an Introduction</a></li></ul><div class=share-buttons><a target=_blank rel="noopener noreferrer" aria-label="share Dynamic Programming for MDP on twitter" href="https://twitter.com/intent/tweet/?text=Dynamic%20Programming%20for%20MDP&url=https%3a%2f%2fZeroAda.github.io%2fposts%2fdp_for_mdp%2f&hashtags=ReinforcementLearning%3aanIntroduction"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM195.519 424.544c135.939.0 210.268-112.643 210.268-210.268.0-3.218.0-6.437-.153-9.502 14.406-10.421 26.973-23.448 36.935-38.314-13.18 5.824-27.433 9.809-42.452 11.648 15.326-9.196 26.973-23.602 32.49-40.92-14.252 8.429-30.038 14.56-46.896 17.931-13.487-14.406-32.644-23.295-53.946-23.295-40.767.0-73.87 33.104-73.87 73.87.0 5.824.613 11.494 1.992 16.858-61.456-3.065-115.862-32.49-152.337-77.241-6.284 10.881-9.962 23.601-9.962 37.088.0 25.594 13.027 48.276 32.95 61.456-12.107-.307-23.448-3.678-33.41-9.196v.92c0 35.862 25.441 65.594 59.311 72.49-6.13 1.686-12.72 2.606-19.464 2.606-4.751.0-9.348-.46-13.946-1.38 9.349 29.426 36.628 50.728 68.965 51.341-25.287 19.771-57.164 31.571-91.8 31.571-5.977.0-11.801-.306-17.625-1.073 32.337 21.15 71.264 33.41 112.95 33.41z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Dynamic Programming for MDP on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&url=https%3a%2f%2fZeroAda.github.io%2fposts%2fdp_for_mdp%2f&title=Dynamic%20Programming%20for%20MDP&summary=Dynamic%20Programming%20for%20MDP&source=https%3a%2f%2fZeroAda.github.io%2fposts%2fdp_for_mdp%2f"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Dynamic Programming for MDP on reddit" href="https://reddit.com/submit?url=https%3a%2f%2fZeroAda.github.io%2fposts%2fdp_for_mdp%2f&title=Dynamic%20Programming%20for%20MDP"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Dynamic Programming for MDP on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fZeroAda.github.io%2fposts%2fdp_for_mdp%2f"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Dynamic Programming for MDP on whatsapp" href="https://api.whatsapp.com/send?text=Dynamic%20Programming%20for%20MDP%20-%20https%3a%2f%2fZeroAda.github.io%2fposts%2fdp_for_mdp%2f"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Dynamic Programming for MDP on telegram" href="https://telegram.me/share/url?text=Dynamic%20Programming%20for%20MDP&url=https%3a%2f%2fZeroAda.github.io%2fposts%2fdp_for_mdp%2f"><svg viewBox="2 2 28 28"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg></a></div></footer></article></main><footer class=footer><span>&copy; 2022 <a href=https://ZeroAda.github.io/>Orange</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://git.io/hugopapermod rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg></a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>