<!doctype html><html lang=en dir=auto><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>How to make decisions in a bandit game? | Orange</title><meta name=keywords content="Reinforcement Learning: an Introduction"><meta name=description content="short notes for sequencial decision making"><meta name=author content="Orange"><link rel=canonical href=https://ZeroAda.github.io/posts/sequence_decision/><link crossorigin=anonymous href=/assets/css/stylesheet.min.c3e9e312489cf2251cb48d39a0c7b446b10f09c27a8f74cb80b221ed1fb07203.css integrity="sha256-w+njEkic8iUctI05oMe0RrEPCcJ6j3TLgLIh7R+wcgM=" rel="preload stylesheet" as=style><script defer crossorigin=anonymous src=/assets/js/highlight.min.e85ad0406048e8176e1c7661b25d5c69297ddfe41dc4124cf75ecb99a4f7b3d1.js integrity="sha256-6FrQQGBI6BduHHZhsl1caSl93+QdxBJM917LmaT3s9E=" onload=hljs.initHighlightingOnLoad()></script>
<link rel=icon href=https://ZeroAda.github.io/apple-touch-icon.png><link rel=icon type=image/png sizes=16x16 href=https://ZeroAda.github.io/apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=https://ZeroAda.github.io/apple-touch-icon.png><link rel=apple-touch-icon href=https://ZeroAda.github.io/apple-touch-icon.png><link rel=mask-icon href=https://ZeroAda.github.io/apple-touch-icon.png><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--hljs-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.css integrity=sha384-KiWOvVjnN8qwAZbuQyWDIbfCLFhLXNETzBQjA/92pIowpC0d2O3nppDGQVgwd2nB crossorigin=anonymous><script defer src=https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/katex.min.js integrity=sha384-0fdwu/T/EQMsQlrHCCHoH10pkPLlKA1jL5dFyUOvB3lfeT2540/2g6YgSi2BL14p crossorigin=anonymous></script>
<script defer src=https://cdn.jsdelivr.net/npm/katex@0.15.3/dist/contrib/auto-render.min.js integrity=sha384-+XBljXPPiv+OzfbB3cVmLHf4hdUFHlWNZN5spNQ7rmHTXpd7WvJum6fIACpNNfIR crossorigin=anonymous></script>
<script>document.addEventListener("DOMContentLoaded",function(){renderMathInElement(document.body,{delimiters:[{left:"$$",right:"$$",display:!0},{left:"$",right:"$",display:!1},{left:"\\(",right:"\\)",display:!1},{left:"\\[",right:"\\]",display:!0}],throwOnError:!1})})</script><script type=application/javascript>var doNotTrack=!1;doNotTrack||(function(e,t,n,s,o,i,a){e.GoogleAnalyticsObject=o,e[o]=e[o]||function(){(e[o].q=e[o].q||[]).push(arguments)},e[o].l=1*new Date,i=t.createElement(n),a=t.getElementsByTagName(n)[0],i.async=1,i.src=s,a.parentNode.insertBefore(i,a)}(window,document,"script","https://www.google-analytics.com/analytics.js","ga"),ga("create","UA-123-45","auto"),ga("send","pageview"))</script><meta property="og:title" content="How to make decisions in a bandit game?"><meta property="og:description" content="short notes for sequencial decision making"><meta property="og:type" content="article"><meta property="og:url" content="https://ZeroAda.github.io/posts/sequence_decision/"><meta property="og:image" content="https://ZeroAda.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta property="article:section" content="posts"><meta property="article:published_time" content="2022-06-28T00:00:00+00:00"><meta property="article:modified_time" content="2022-06-28T00:00:00+00:00"><meta property="og:site_name" content="Orange"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://ZeroAda.github.io/%3Clink%20or%20path%20of%20image%20for%20opengraph,%20twitter-cards%3E"><meta name=twitter:title content="How to make decisions in a bandit game?"><meta name=twitter:description content="short notes for sequencial decision making"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://ZeroAda.github.io/posts/"},{"@type":"ListItem","position":2,"name":"How to make decisions in a bandit game?","item":"https://ZeroAda.github.io/posts/sequence_decision/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"How to make decisions in a bandit game?","name":"How to make decisions in a bandit game?","description":"short notes for sequencial decision making","keywords":["Reinforcement Learning: an Introduction"],"articleBody":"Suppose you are faced with a 10-arm bandit. For each arm, it has a distribution of reward. Your goal is to get as much reward as possible. But the problem is, you do not know the distribution (mean, variance, etc.). Your only method is trial-and-error, i.e. learn by trying. Now, I think that I should first evaluate my action and then take it by strategy.\nHow to evaluate an action? 1. Action value method I evaluate each action by compute its mean reward till now.\n$$ Q_{n+1} = \\frac{1}{n}\\sum_{i=1}^{n}R_i $$\n2. Incremental Method for stationary problems It can be achieved by incrementation, so that I can choose\n$$ Q_{n+1} = Q_{n} + \\frac{1}{n} (R_n - Q_n) $$\nIt is an update of Q value. At the beginning, it encourages exploration; as we step forward, we assign less and less weight to the current reward.\n3. Incremental Method for nonstationary problems Method 2 is effective in stationary problem, i.e. environment (state-action-reward) stay fixed all the time. If there is a sudden change in environment, we should use Method 3, where $\\alpha$ is the step size.\nStep size is a trade-off between past and current. If $\\alpha$ is larger, we are more adaptive to the environment change and quickly get to the true value. But it also makes q value senstive to environment. If $\\alpha$ is small, it gets to the true value too slowly. $$ Q_{n+1} = Q_{n} + \\alpha (R_n - Q_n) = (1-\\alpha)^n Q1 + \\sum_{i=1}^{n}\\alpha (1-\\alpha)^{n-i} R_i $$ where $\\alpha$ should satisfy $\\sum_{n=1}^{\\infty}a_n(a) = \\infty$ and $\\sum_{n=1}^{\\infty}\\alpha_n^2 (a) \u003c\\infty$\n4. Optimistic Initial Values Instead of zero initialization of Q, we set Q to a value (better if it is higher than all rewards). It can encourage exploration with the use of greedy policy. Imagine you set q to 4, and at the beginning, rewards is all below 4 for each action, so q is negative. Then we will jump to other actions. However, it only works well in stationary problem since it is a temporary drive for exploration.\nHow to take an action? It is a problem of “policy” given q value. We often need to balance exploration and exploitation.\ngreedy action take the action that leads to best q\ndef greedy(q_values): top_value = float(\"-inf\") ties = [] for i in range(len(q_values)): # if a value in q_values is greater than the highest value update top and reset ties to zero if q_values[i] \u003e top_value: top_value=q_values[i] ties=[i] # if a value is equal to top value add the index to ties elif q_values[i] == top_value: ties.append(i) # return a random selection from ties. return np.random.choice(ties) epsilon greedy With probability $\\epsilon$, take random action;\nwith $p=1-\\epsilon$, take greedy action\ndef greedy(q_values): if np.random.random() \u003c self.epsilon: # random current_action=np.random.randint(len(self.arm_count)) else: current_action=greedy(self.q_values) return current_action We need to choose an optimal parameter. 0.1 is a good value. Upper-Confidence-Bound action selection The intuition behind this method is we want to select actions with the highest upper bound of confidence interval.\n(from Coursera Fundamental of RL (Upper-Confidence Bound (UCB) Action Selection) So we need to compute $mean + \\frac{1}{2}confidence interval$. However, we do not know the distribution. So we should estimate upper confidence bound using the formula:\n$$ A_t = argmax_a [Q_t(a) + c \\sqrt{\\frac{\\ln t}{N_t(a)}}] $$ where the first half is exploitation strategy, the second half is exploration startegy; $c$ will control the balance; $t$ is the number of trials you have done and $N_t(a)$ is the number of times you select the action a. When $N_{t}(a)$ increase, the uncertainty decreases; when t increase but N not change, the uncertainty increases.\nReference Reinforcement Learning: An Introduction Chapter 2: 2.4,2.5,2.6\nCoursera: Fundamental of RL\n","wordCount":"610","inLanguage":"en","datePublished":"2022-06-28T00:00:00Z","dateModified":"2022-06-28T00:00:00Z","author":{"@type":"Person","name":"Orange"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://ZeroAda.github.io/posts/sequence_decision/"},"publisher":{"@type":"Organization","name":"Orange","logo":{"@type":"ImageObject","url":"https://ZeroAda.github.io/apple-touch-icon.png"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://ZeroAda.github.io/ accesskey=h title="Orange (Alt + H)"><img src=https://ZeroAda.github.io/apple-touch-icon.png alt=logo aria-label=logo height=35>Orange</a>
<span class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></span></div><ul id=menu><li><a href=https://ZeroAda.github.io/about/ title=About><span>About</span></a></li><li><a href=https://ZeroAda.github.io/posts/ title=Posts><span>Posts</span></a></li><li><a href=https://ZeroAda.github.io/projects/ title=Projects><span>Projects</span></a></li><li><a href=https://ZeroAda.github.io/tags/ title=Tags><span>Tags</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://ZeroAda.github.io/>Home</a>&nbsp;»&nbsp;<a href=https://ZeroAda.github.io/posts/>Posts</a></div><h1 class=post-title>How to make decisions in a bandit game?</h1><div class=post-description>short notes for sequencial decision making</div><div class=post-meta><span title='2022-06-28 00:00:00 +0000 UTC'>June 28, 2022</span>&nbsp;·&nbsp;3 min&nbsp;·&nbsp;Orange</div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><ul><li><a href=#how-to-evaluate-an-action aria-label="How to evaluate an action?">How to evaluate an action?</a><ul><li><a href=#1-action-value-method aria-label="1. Action value method">1. Action value method</a></li><li><a href=#2-incremental-method-for-stationary-problems aria-label="2. Incremental Method for stationary problems">2. Incremental Method for stationary problems</a></li><li><a href=#3-incremental-method-for-nonstationary-problems aria-label="3. Incremental Method for nonstationary problems">3. Incremental Method for nonstationary problems</a></li><li><a href=#4-optimistic-initial-values aria-label="4. Optimistic Initial Values">4. Optimistic Initial Values</a></li></ul></li><li><a href=#how-to-take-an-action aria-label="How to take an action?">How to take an action?</a><ul><li><a href=#greedy-action aria-label="greedy action">greedy action</a></li><li><a href=#epsilon-greedy aria-label="epsilon greedy">epsilon greedy</a></li><li><a href=#upper-confidence-bound-action-selection aria-label="Upper-Confidence-Bound action selection">Upper-Confidence-Bound action selection</a></li></ul></li><li><a href=#reference aria-label=Reference>Reference</a></li></ul></div></details></div><div class=post-content><p>Suppose you are faced with a 10-arm bandit. For each arm, it has a distribution of reward. Your goal is to get as much reward as possible. But the problem is, you do not know the distribution (mean, variance, etc.). Your only method is trial-and-error, i.e. learn by trying. Now, I think that I should first evaluate my action and then take it by strategy.</p><h2 id=how-to-evaluate-an-action>How to evaluate an action?<a hidden class=anchor aria-hidden=true href=#how-to-evaluate-an-action>#</a></h2><h3 id=1-action-value-method>1. Action value method<a hidden class=anchor aria-hidden=true href=#1-action-value-method>#</a></h3><p>I evaluate each action by compute its mean reward till now.</p><p>$$
Q_{n+1} = \frac{1}{n}\sum_{i=1}^{n}R_i
$$</p><h3 id=2-incremental-method-for-stationary-problems>2. Incremental Method for stationary problems<a hidden class=anchor aria-hidden=true href=#2-incremental-method-for-stationary-problems>#</a></h3><p>It can be achieved by incrementation, so that I can choose</p><p>$$
Q_{n+1} = Q_{n} + \frac{1}{n} (R_n - Q_n)
$$</p><p>It is an update of Q value. At the beginning, it <strong>encourages exploration</strong>; as we step forward, we assign less and less weight to the current reward.</p><h3 id=3-incremental-method-for-nonstationary-problems>3. Incremental Method for nonstationary problems<a hidden class=anchor aria-hidden=true href=#3-incremental-method-for-nonstationary-problems>#</a></h3><p>Method 2 is effective in stationary problem, i.e. environment (state-action-reward) stay fixed all the time. If there is a sudden change in environment, we should use Method 3, where $\alpha$ is the step size.</p><p><img loading=lazy src=/img/rl/stepsize1.png alt>
Step size is a trade-off between past and current. If $\alpha$ is larger, we are more adaptive to the environment change and quickly get to the true value. But it also makes q value senstive to environment. If $\alpha$ is small, it gets to the true value too slowly.
<img loading=lazy src=/img/rl/stepsize2.png alt></p><p>$$
Q_{n+1} = Q_{n} + \alpha (R_n - Q_n) = (1-\alpha)^n Q1 + \sum_{i=1}^{n}\alpha (1-\alpha)^{n-i} R_i
$$
where $\alpha$ should satisfy $\sum_{n=1}^{\infty}a_n(a) = \infty$ and $\sum_{n=1}^{\infty}\alpha_n^2 (a) &lt;\infty$</p><h3 id=4-optimistic-initial-values>4. Optimistic Initial Values<a hidden class=anchor aria-hidden=true href=#4-optimistic-initial-values>#</a></h3><p>Instead of zero initialization of Q, we set Q to a value (better if it is higher than all rewards). It can encourage exploration with the use of greedy policy. Imagine you set q to 4, and at the beginning, rewards is all below 4 for each action, so q is negative. Then we will jump to other actions. However, it only works well in <em>stationary</em> problem since it is a temporary drive for exploration.</p><h2 id=how-to-take-an-action>How to take an action?<a hidden class=anchor aria-hidden=true href=#how-to-take-an-action>#</a></h2><p>It is a problem of &ldquo;policy&rdquo; given q value. We often need to balance exploration and exploitation.</p><h3 id=greedy-action>greedy action<a hidden class=anchor aria-hidden=true href=#greedy-action>#</a></h3><p>take the action that leads to best q</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>greedy</span>(q_values):
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    top_value <span style=color:#f92672>=</span> float(<span style=color:#e6db74>&#34;-inf&#34;</span>)
</span></span><span style=display:flex><span>    ties <span style=color:#f92672>=</span> []
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>for</span> i <span style=color:#f92672>in</span> range(len(q_values)):
</span></span><span style=display:flex><span>        <span style=color:#75715e># if a value in q_values is greater than the highest value update top and reset ties to zero</span>
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>if</span> q_values[i] <span style=color:#f92672>&gt;</span> top_value:
</span></span><span style=display:flex><span>            top_value<span style=color:#f92672>=</span>q_values[i]
</span></span><span style=display:flex><span>            ties<span style=color:#f92672>=</span>[i]
</span></span><span style=display:flex><span>        <span style=color:#75715e># if a value is equal to top value add the index to ties</span>
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>elif</span> q_values[i] <span style=color:#f92672>==</span> top_value:
</span></span><span style=display:flex><span>            ties<span style=color:#f92672>.</span>append(i)
</span></span><span style=display:flex><span>        <span style=color:#75715e># return a random selection from ties.</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> np<span style=color:#f92672>.</span>random<span style=color:#f92672>.</span>choice(ties)
</span></span></code></pre></div><h3 id=epsilon-greedy>epsilon greedy<a hidden class=anchor aria-hidden=true href=#epsilon-greedy>#</a></h3><p>With probability $\epsilon$, take random action;</p><p>with $p=1-\epsilon$, take greedy action</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>greedy</span>(q_values):
</span></span><span style=display:flex><span>     <span style=color:#66d9ef>if</span> np<span style=color:#f92672>.</span>random<span style=color:#f92672>.</span>random() <span style=color:#f92672>&lt;</span> self<span style=color:#f92672>.</span>epsilon:
</span></span><span style=display:flex><span>            <span style=color:#75715e># random</span>
</span></span><span style=display:flex><span>            current_action<span style=color:#f92672>=</span>np<span style=color:#f92672>.</span>random<span style=color:#f92672>.</span>randint(len(self<span style=color:#f92672>.</span>arm_count))
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>else</span>:
</span></span><span style=display:flex><span>            current_action<span style=color:#f92672>=</span>greedy(self<span style=color:#f92672>.</span>q_values)
</span></span><span style=display:flex><span>        
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> current_action
</span></span></code></pre></div><p>We need to choose an optimal parameter. 0.1 is a good value.
<img loading=lazy src=/img/rl/epsilon.png alt></p><h3 id=upper-confidence-bound-action-selection>Upper-Confidence-Bound action selection<a hidden class=anchor aria-hidden=true href=#upper-confidence-bound-action-selection>#</a></h3><p>The intuition behind this method is we want to select actions with the highest upper bound of confidence interval.</p><p><img loading=lazy src=/img/rl/ucb.png alt="from Coursera Fundamental of RL (Upper-Confidence Bound (UCB) Action Selection)">
(from Coursera Fundamental of RL (Upper-Confidence Bound (UCB) Action Selection)
So we need to compute $mean + \frac{1}{2}confidence interval$. However, we do not know the distribution. So we should estimate upper confidence bound using the formula:</p><p>$$
A_t = argmax_a [Q_t(a) + c \sqrt{\frac{\ln t}{N_t(a)}}]
$$
where the first half is <strong>exploitation</strong> strategy, the second half is <strong>exploration</strong> startegy; $c$ will control the balance; $t$ is the number of trials you have done and $N_t(a)$ is the number of times you select the action a. When $N_{t}(a)$ increase, the uncertainty decreases; when t increase but N not change, the uncertainty increases.</p><h2 id=reference>Reference<a hidden class=anchor aria-hidden=true href=#reference>#</a></h2><p><em>Reinforcement Learning: An Introduction</em> Chapter 2: 2.4,2.5,2.6</p><p>Coursera: <em>Fundamental of RL</em></p></div><footer class=post-footer><ul class=post-tags><li><a href=https://ZeroAda.github.io/tags/reinforcement-learning-an-introduction/>Reinforcement Learning: an Introduction</a></li></ul><nav class=paginav><a class=next href=https://ZeroAda.github.io/posts/fourier_transform/><span class=title>Next Page »</span><br><span>Fourier Transform</span></a></nav><div class=share-buttons><a target=_blank rel="noopener noreferrer" aria-label="share How to make decisions in a bandit game? on twitter" href="https://twitter.com/intent/tweet/?text=How%20to%20make%20decisions%20in%20a%20bandit%20game%3f&url=https%3a%2f%2fZeroAda.github.io%2fposts%2fsequence_decision%2f&hashtags=ReinforcementLearning%3aanIntroduction"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM195.519 424.544c135.939.0 210.268-112.643 210.268-210.268.0-3.218.0-6.437-.153-9.502 14.406-10.421 26.973-23.448 36.935-38.314-13.18 5.824-27.433 9.809-42.452 11.648 15.326-9.196 26.973-23.602 32.49-40.92-14.252 8.429-30.038 14.56-46.896 17.931-13.487-14.406-32.644-23.295-53.946-23.295-40.767.0-73.87 33.104-73.87 73.87.0 5.824.613 11.494 1.992 16.858-61.456-3.065-115.862-32.49-152.337-77.241-6.284 10.881-9.962 23.601-9.962 37.088.0 25.594 13.027 48.276 32.95 61.456-12.107-.307-23.448-3.678-33.41-9.196v.92c0 35.862 25.441 65.594 59.311 72.49-6.13 1.686-12.72 2.606-19.464 2.606-4.751.0-9.348-.46-13.946-1.38 9.349 29.426 36.628 50.728 68.965 51.341-25.287 19.771-57.164 31.571-91.8 31.571-5.977.0-11.801-.306-17.625-1.073 32.337 21.15 71.264 33.41 112.95 33.41z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share How to make decisions in a bandit game? on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&url=https%3a%2f%2fZeroAda.github.io%2fposts%2fsequence_decision%2f&title=How%20to%20make%20decisions%20in%20a%20bandit%20game%3f&summary=How%20to%20make%20decisions%20in%20a%20bandit%20game%3f&source=https%3a%2f%2fZeroAda.github.io%2fposts%2fsequence_decision%2f"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share How to make decisions in a bandit game? on reddit" href="https://reddit.com/submit?url=https%3a%2f%2fZeroAda.github.io%2fposts%2fsequence_decision%2f&title=How%20to%20make%20decisions%20in%20a%20bandit%20game%3f"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share How to make decisions in a bandit game? on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fZeroAda.github.io%2fposts%2fsequence_decision%2f"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share How to make decisions in a bandit game? on whatsapp" href="https://api.whatsapp.com/send?text=How%20to%20make%20decisions%20in%20a%20bandit%20game%3f%20-%20https%3a%2f%2fZeroAda.github.io%2fposts%2fsequence_decision%2f"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share How to make decisions in a bandit game? on telegram" href="https://telegram.me/share/url?text=How%20to%20make%20decisions%20in%20a%20bandit%20game%3f&url=https%3a%2f%2fZeroAda.github.io%2fposts%2fsequence_decision%2f"><svg viewBox="2 2 28 28"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg></a></div></footer></article></main><footer class=footer><span>&copy; 2022 <a href=https://ZeroAda.github.io/>Orange</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://git.io/hugopapermod rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg></a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>